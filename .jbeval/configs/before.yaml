---
evaluator:
  type: "llm_judge"
  params:
    max_votes: 3
    max_concurrency: 5
model:
  params:
    model_name: "gpt-4o-mini"
    temperature: 0.0
  api_key: null
prompt:
  template: "You are an impartial AI judge specializing in evaluating semantic equivalence.\
    \ Your role is to assess whether two outputs convey substantially the same core\
    \ information and meaning, even if expressed differently, with allowances for\
    \ variations.\nYou will receive:\n\nA Task description.\nA Reference Output (ground\
    \ truth).\nA Model Output to evaluate.\n\nTo evaluate:\nThink step-by-step in\
    \ <thinking> tags:</thinking>\n\nIdentify the key facts, ideas, concepts, and\
    \ essential information in the Reference Output.\nIdentify the key facts, ideas,\
    \ concepts, and essential information in the Model Output.\nCompare them for semantic\
    \ similarity: Do they substantially align in overall meaning and core content?\
    \ Allow for paraphrasing, synonyms, reordered elements, different structures,\
    \ casing, whitespace, minor additions, omissions, or non-critical details. Focus\
    \ on whether the essence is preserved, not on exact wording or precision.\n\n\
    Assign one of two integer scores: 0 or 1, according to:\n\n1: Substantially matches\
    \ semantically (conveys mostly the same key information and meaning, with allowances\
    \ for variations that don't alter the core essence).\n0: Does not substantially\
    \ match semantically (significant deviations, missing core information, major\
    \ inaccuracies, or fundamentally different meaning).\n\nHere are examples to guide\
    \ your evaluation:\n<examples>\n<example1>\nTask: Summarize the plot of the fairy\
    \ tale.\nReference Output: A young girl visits her grandmother but encounters\
    \ a wolf disguised as her.\nModel Output: The story involves a child going to\
    \ see her grandma, only to find a wolf pretending to be her.\n</example1>\nExpected\
    \ Response: {\"score\": 1, \"explanation\": \"Substantially equivalent; same core\
    \ events and characters, just rephrased.\"}\n<example2>\nTask: List the capitals\
    \ of European countries.\nReference Output: France - Paris, Germany - Berlin,\
    \ Italy - Rome.\nModel Output: France - Paris, Spain - Madrid, UK - London.\n\
    </example2>\nExpected Response: {\"score\": 0, \"explanation\": \"Does not match;\
    \ includes different countries and capitals, altering core information.\"}\n<example3>\n\
    Task: Explain photosynthesis.\nReference Output: Plants convert sunlight, carbon\
    \ dioxide, and water into glucose and oxygen using chlorophyll.\nModel Output:\
    \ Through chlorophyll, vegetation transforms CO2, H2O, and solar energy into sugar\
    \ and O2.\n</example3>\nExpected Response: {\"score\": 1, \"explanation\": \"\
    Conveys the same scientific process and components, despite varied wording, abbreviations,\
    \ and synonyms.\"}\n<example4>\nTask: Describe the benefits of exercise.\nReference\
    \ Output: Regular exercise improves cardiovascular health, strengthens muscles,\
    \ and boosts mental well-being.\nModel Output: Working out routinely enhances\
    \ heart function, builds muscle strength, reduces stress, and promotes better\
    \ mood.\n</example4>\nExpected Response: {\"score\": 1, \"explanation\": \"Substantially\
    \ matches core benefits with paraphrasing and minor expansions, preserving the\
    \ essence.\"}\n</examples>\n\nRespond only with a JSON object containing \"score\"\
    \ (0 or 1) and \"explanation\" (a brief justification). Do not include the <thinking>\
    \ tags in your final response.</thinking>\nNow evaluate:\n<thinking>\n[Your step-by-step\
    \ reasoning here, but it will not appear in the output]\n</thinking>\nTask: {input}\n\
    Reference Output: {output_expected}\nModel Output: {output_gen}"
  input_variables:
    input: "input"
    output_expected: "output_expected"
    output_gen: "output_gen"
output:
  dir: ".jbeval/eval"
extra:
  dataset_path: ".jbeval/eval/before.json"
