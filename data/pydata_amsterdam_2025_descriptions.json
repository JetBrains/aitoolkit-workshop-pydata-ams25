{
  "77812": {
    "title": "Next-Level Retrieval in RAG: Techniques and Tools for Enhanced Performance",
    "speakers": [
      "Mahima Arora",
      "Aarti Jha"
    ],
    "type": "Tutorial",
    "abstract": "Retrieval-Augmented Generation (RAG) systems rely heavily on the quality of the retrieval process to generate accurate and contextually relevant outputs. In this 90-minute tutorial, we explore practical techniques to enhance retrieval across three key stages: pre-retrieval, mid-retrieval, and post-retrieval. Participants will learn how to optimize data preparation, query strategies, reranking, and evaluation to significantly improve the performance of RAG systems. A real-world case study will guide attendees through implementing these methods in a complete retrieval workflow.",
    "description_text": "The tutorial provides a structured approach to optimizing retrieval in RAG pipelines. We'll cover: * Pre-retrieval optimization: how to preprocess and structure data, choose appropriate embeddings, and design efficient indexing and caching mechanisms. * Mid-retrieval optimization: strategies for query expansion, context-aware reformulation, and reranking retrieved documents for relevance. * Post-retrieval optimization: methods for refining and filtering results, incorporating user feedback, and continuously evaluating system performance. Participants will explore practical techniques and open-source tools to operationalize enhanced retrieval at each of these stages. We\u2019ll demonstrate the full workflow through a detailed case study, showing how to implement different techniques at each stage to build a more efficient and intelligent retrieval pipeline. #### Outline: * Introduction to Retrieval in RAG Systems * Case Study Walkthrough: Applying All Stages * Pre-Retrieval Optimization * Data preprocessing * Embedding techniques * Indexing strategies * Pre-filtering and caching * Mid-Retrieval Optimization * Query reformulation and expansion * Contextual query reframing * Recursive summarization frameworks * Reranking techniques * Distributed retrieval methods * Post-Retrieval Optimization * Result filtering and refinement * Contextual relevance using chat history * Feedback loops and active learning * Evaluation metrics and continuous monitoring * Q&A and Wrap-up Background Knowledge Required: * Familiarity with Python * Basic understanding of LLMs Target Audience: ML engineers, data scientists, developers working with LLMs in production, and anyone looking to learn how to build robust AI workflows using open source tools."
  },
  "79192": {
    "title": "Building AI Agents With Observability Tooling in PyCharm",
    "speakers": [
      "Yaroslav Sokolov",
      "Lenar Sharipov"
    ],
    "type": "Tutorial",
    "abstract": "As AI-powered agents and workflows grow in complexity, understanding their internal behavior becomes critical. In this hands-on workshop, you\u2019ll build an agent and explore how observability tooling in PyCharm can help you trace, inspect, and debug its behavior at every stage \u2013 without having to leave the IDE.",
    "description_text": "This workshop introduces a practical workflow for building AI agents using LangGraph, with a focus on observability tooling in PyCharm to manage growing complexity. As the agent\u2019s logic becomes more advanced, we\u2019ll show how tracing and inspection tools help developers understand behavior, diagnose issues, and stay in control of what the agent is doing. In this workshop, you will: Build AI agents using LangGraph. Use observability tooling in PyCharm to trace and debug agent behavior. Make use of your confident Python skills and basic LLM knowledge."
  },
  "77612": {
    "title": "Bridging the Gap: Building Robust, Tool-Integrated LLM Applications with the Model Context Protocol",
    "speakers": [
      "Adam Hill",
      "Shourya Sharma"
    ],
    "type": "Tutorial",
    "abstract": "Large Language Models (LLMs) are unlocking transformative capabilities \u2014 but integrating them into complex, real-world applications remains a major challenge. Simple prompting isn\u2019t enough when dynamic interaction with tools, structured data, and live context is required. This workshop introduces the Model Context Protocol (MCP), an emerging open standard designed to simplify and standardise this integration. Aimed at forward-thinking developers and technologists, this hands-on session will equip participants with practical skills to build intelligent, modular, and extensible LLM-native applications using MCP.",
    "description_text": "Dive into the next evolution of LLM application development with the Model Context Protocol. As LLMs become more capable, the need for a consistent way to connect them with external tools and data is increasingly urgent. This interactive workshop explores MCP as a breakthrough approach to building tool-aware, context-rich AI systems. We\u2019ll cover: - Core Concepts of MCP: Understand MCP's client-server architecture and how it enables structured, contextual reasoning \u2014 think of it as \u201cUSB-C for AI.\u201d Learn how tools and resources are exposed, discovered, and invoked via the protocol. - Development Environment Setup: Get hands-on with a modern Python development stack, including FastMCP, Typer, and Streamlit. Configure access to an LLM backend using either the OpenAI API or a local Ollama instance. - Building MCP Servers & Tools: Learn to expose functions and resources through an MCP server. We\u2019ll walk through a real example \u2014 a Wikipedia search-and-summarisation tool \u2014 to demonstrate how to define capabilities and serve them in a standards-compliant way. - Client Development (CLI & Web): Build robust clients using Typer for command-line interfaces and Streamlit for web apps. Learn how to discover server tools, invoke them, and manage stateful interactions. - LLM-Orchestrated Interactions: Go beyond prompt engineering. See how LLMs can autonomously select and chain tools via MCP to solve complex tasks. We\u2019ll demonstrate orchestration logic that turns LLMs into intelligent agents \u2014 not just text predictors. - Build an AI Research Assistant: Apply your skills by building a working prototype \u2014 an AI Research Assistant. This tool will take natural language queries, use an LLM to plan a response, and call MCP tools to search and summarise data in context. By the end of this workshop, participants will have a working understanding of MCP, a functioning toolchain, and a clear roadmap to building next-generation AI applications. Who Should Attend: Developers, data scientists, and technical practitioners ready to move beyond basic prompting and explore advanced, structured LLM integration. Requirements: To get the most from this workshop, attendees should have: - Python 3.11+ installed - Docker (recommended for running the MCP server) - Access to an LLM backend (OpenAI API or local Ollama) - Basic familiarity with Python and the command line"
  },
  "77813": {
    "title": "Event-Driven AI Agent Workflows with Dapr",
    "speakers": [
      "Dana Arsovska",
      "Marc Duiker"
    ],
    "type": "Tutorial",
    "abstract": "As AI systems evolve, the need for robust infrastructure increases. Enter Dapr Agents: an open-source framework for creating production-grade AI agent systems. Built on top of the [Dapr](https://github.com/dapr/dapr) framework, Dapr Agents empowers developers to build intelligent agents capable of collaborating in complex workflows - leveraging Large Language Models (LLMs), durable state, built-in observability, and resilient execution patterns. This workshop will walk through the framework\u2019s core components and through practical examples demonstrate how it solves real-world challenges.",
    "description_text": "Managing communication, state, and resiliency between distributed systems remains complex - especially when building multi-agent systems that can reason, act, and collaborate using LLMs. Dapr Agents addresses this gap by bringing durability, observability, and event-driven design to AI applications. In this workshop, we will create an event-driven workflow with multiple autonomous agents in Python and explore Dapr Agents' key features. **Target Audience** Engineers and data scientists who are exploring or working with LLM-based systems and are excited to build intelligent, multi-agent applications. Attendees should have intermediate Python knowledge. Experience with Dapr is not required, but some understanding of distributed systems, LLMs, or async programming will be helpful. We will not explain every concept in depth, but we will provide resources so you can dig deeper on your own as needed. Here are the things you should install before the start of this workshop: - [Docker](https://docs.docker.com/desktop/) - Python 3.10 or above - virtual environment (recommended uv >= 0.4.25) - [Dapr CLI](https://docs.dapr.io/getting-started/install-dapr-cli/) **Takeaways** You\u2019ll learn how to: - Build agents using Python and Dapr - Connect these agents through event-driven pub/sub messaging - Persist agent state using Dapr\u2019s state management - Design resilient, reactive multi-agent workflows - Observe and debug your system **Outline** - Introduction (20 mins): 1. What is Dapr? 2. What are LLM Agents? 3. Introducing Dapr Agents 4. Workshop overview - Environment setup (5 mins) 1. Install dependencies 2. Clone workshop repo and verify setup - Develop event-driven workflows with multiple autonomous agents (65 mins) 1. Create simple agents (exercise) 2. Orchestrate workflow using different workflow types (exercise) 3. Run the multi-agent system (exercise) 4. Recap key learnings & share additional materials and next steps"
  },
  "77831": {
    "title": "Meet Docling: The \u201cPandas\u201d for document AI",
    "speakers": [
      "Mingxuan Zhao",
      "Panos Vagenas"
    ],
    "type": "Tutorial",
    "abstract": "A workshop session to show you the basics on how to use Docling to enhance document ingestion in your AI workflow.",
    "description_text": "With the rapid rise of AI, developers need better ways to transform complex documents into structured data ready for model training and inference. Enter Docling, an open source Python package that's quickly becoming the go-to for document parsing and export. In just a few months, Docling has earned over 25,000 GitHub stars and is already reshaping how developers approach document AI. In this session, you'll get an in-depth introduction to Docling and how it can streamline your AI workflow, and get a chance to walk through a hands on workshop to create your first custom doc ingestion pipeline with Docling. Key features include: Broad format support: Easily convert PDFs, DOCX, PPTX, HTML, images, and Markdown into structured Markdown or JSON. Deep document understanding: Accurately capture page layouts, reading order, and tables\u2014essential for complex document analysis. AI integration: Use the DoclingDocument format with frameworks like LlamaIndex, LangChain, and InstructLab to power RAG, QA, and LLM training. OCR support: Extract data from scanned or image-based documents. Developer friendly CLI: Process documents quickly and consistently with a simple command-line interface. This workshop will require users to have experience with Python programming and LLMs. It will be presented in Jupyter notebook format and will be accessible and runnable in Google collab, ensuring all participants devices will work for the session."
  },
  "77828": {
    "title": "Understand your data with Knowledge Graphs",
    "speakers": [
      "Martin O'Hanlon"
    ],
    "type": "Tutorial",
    "abstract": "Graph databases give the same importance to relationships as they do to data. Knowledge graphs allows you to uncover insights in your data and efficiently explore the relationships in your data.",
    "description_text": "In this hands-on workshop you will learn how graph databases can help you find understand relationships. You will: - Learn about graph theory, the elements of a graph database, and when a graph database is a good choice for your use case. - Explore a graph database, learn the basics of the query language Cypher, and use Cypher to read and write data. - Import your data into graph database."
  },
  "77814": {
    "title": "Grounding LLMs on Solid Knowledge: Assessing and Improving Knowledge Graph Quality in GraphRAG Applications",
    "speakers": [
      "Panos Alexopoulos"
    ],
    "type": "Tutorial",
    "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances large language models (LLMs) by grounding their responses in structured knowledge graphs, offering more accurate, domain-specific, and explainable outputs. However, many of the graphs used in these pipelines are automatically generated or loosely assembled, and often lack the semantic structure, consistency, and clarity required for reliable grounding. The result is misleading retrieval, vague or incomplete answers, and hallucinations that are difficult to trace or fix. This hands-on tutorial introduces a practical approach to evaluating and improving knowledge graph quality in GraphRAG applications. We\u2019ll explore common failure patterns, walk through real-world examples, and share a reusable checklist of features that make a graph \u201cAI-ready.\u201d Participants will learn methods for identifying gaps, inconsistencies, and modeling issues that prevent knowledge graphs from effectively supporting LLMs, and apply simple fixes to improve grounding and retrieval performance in their own projects.",
    "description_text": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances large language models (LLMs) by grounding their responses in structured knowledge graphs, offering more accurate, domain-specific, and explainable outputs. However, many of the graphs used in these pipelines are automatically generated or loosely assembled, and often lack the semantic structure, consistency, and clarity required for reliable grounding. The result is misleading retrieval, vague or incomplete answers, and hallucinations that are difficult to trace or fix. This hands-on tutorial introduces a practical framework for evaluating and improving knowledge graph quality in GraphRAG systems. We combine lecture-based instruction with a coding case study to explore the structural and semantic pitfalls of weak graphs, and demonstrate how to identify and address them. By the end of the tutorial, participants will be able to: - Understand the importance of high-quality KGs in GraphRAG systems and their role in grounding LLM outputs. - Identify common problems in LLM-generated or lightly modeled graphs. - Apply methods and heuristics for KG quality assessment, including validation, error detection, and refinement techniques. - Implement strategies to improve graph structure and content for better AI performance. Format & Requirements: - Lecture + live coding walk-through (Jupyter notebooks in Python) - Materials will be shared via GitHub - Prior exposure to RAG or knowledge graphs is helpful, but not required Tentative Outline (90 mins): - 0\u201315 min \u2013 Introduction to GraphRAG & role of KGs in grounding - 15\u201335 min \u2013 Common issues in LLM-generated or low-quality graphs - 35\u201360 min \u2013 KG quality: assessment and improvement techniques - 60\u201385 min \u2013 Live coding walkthrough: identify and fix issues in a real-world knowledge graph - 85\u201390 min \u2013 Wrap-up + Q&A This tutorial is ideal for data scientists, ML engineers, and AI developers looking to build more robust, explainable, and effective GraphRAG systems."
  },
  "77815": {
    "title": "Listen: A Practical Introduction to Data Sonification",
    "speakers": [
      "Tomek Roszczynialski"
    ],
    "type": "Tutorial",
    "abstract": "Sonification\u2013using sound to represent data\u2013is a niche technique for exploring complex patterns, expanding the sensory dimensions of data analysis, and discovering musical ideas that are otherwise inaccessible. In this hands-on session, participants will learn the ins and outs of building sonification pipelines through practical examples with data from healthcare and physics. We\u2019ll also cover key software design considerations for creating flexible and expressive systems that map data into sound. Whether you're a developer, data scientist, researcher, educator, or artist, this session will help you listen to your data.",
    "description_text": "This tutorial is aimed at people working with data who are familiar with basic components of music, like pitch, melody, harmony, and rhythm, although the ability to play an instrument is not required. The technical part will rely on common libraries in the Python data analysis stack: Numpy, Pandas, Matplotlib, Streamlit, and HuggingFace. The goal of this tutorial is to introduce you to the challenge of creating generative music that is grounded in, and reflects some underlying data structure\u2013whether temporal, spatial, or statistical\u2013while remaining musically coherent and expressive. We\u2019ll cover a number of methods, from straightforward rule-based systems to billion-parameter transformer models, highlighting the tradeoffs between control, complexity, and creative potential across different approaches. During the tutorial you\u2019ll get access to relevant data, and a repository containing end-to-end code examples, including a sonification of healthcare data (ECG signals) and a chaotic physical system (the double pendulum). Using those examples, we\u2019ll explore multiple sonification strategies that can be applied, and multiple aesthetic choices that need to be considered when writing sonification software. ## Outline 1. A brief history of sound, music, and algorithms. (10m) 2. Becoming one with the data \u2013 exploring datasets with all your senses. (15m) 3. The piano keyboard \u2013 absolute minimum of music theory for data scientists. (10m) 4. Finding beauty in the dissonance \u2013 sonification of chaotic motion of the double pendulum. Step-by-step walk through. (20m) 5. Extra beat \u2013 sonification of ECG data acquired from patients with various types of arrhythmia. Step-by-step walk through. (20m) 6. Team work \u2013 exploring ideas together with the audience. (15m) If you want to experiment with your own sonification ideas during the tutorial (which is encouraged), please bring headphones! \ud83c\udfa7"
  },
  "77051": {
    "title": "Opening notes",
    "speakers": [],
    "type": "Opening notes",
    "abstract": "Opening notes",
    "description_text": "Opening notes"
  },
  "77528": {
    "title": "The agentification of software (has a UX problem)",
    "speakers": [
      "Demetrios Brinkmann"
    ],
    "type": "Keynote",
    "abstract": "The \"agentification\" of software promises a future where we simply tell a machine what we want, and it handles the rest. We have all felt that magic when vibe coding. So why can't every interaction with machines be that magical? Well, what if the dominant chat-based interface is a dead end? This talk explores the significant UX challenges of agentification. We\u2019ll discuss why chat is an over-promising and under-delivering medium, and how we're losing the rich, high-bandwidth context of human communication.",
    "description_text": "."
  },
  "77873": {
    "title": "Should Captain America Still Host Your Data? A Call for Open, EU-Based Data Platforms.",
    "speakers": [
      "Manuel Spierenburg"
    ],
    "type": "Talk",
    "abstract": "When you store data in the cloud, do you know who really controls it? In an era of increasing geopolitical tension and growing awareness around digital sovereignty, Dutch research institutes have already begun repatriating sensitive data from US servers to Dutch-controlled storage. This talk explores the hidden risks behind common cloud choices, from legal access by foreign governments to the ethical implications of supporting politically active tech giants. We\u2019ll look at what it means to own your data, how regional storage might not be enough, and what it takes to build an EU-hosted, open-source data platform stack. If you\u2019re a data engineer, architect, or technology leader who cares about privacy, control, and sustainable infrastructure, this talk will equip you with the insight\u2014and motivation\u2014to make different choices.",
    "description_text": "Cloud infrastructure is a foundational part of modern data platforms\u2014but are we fully aware of the trade-offs we make when choosing a provider? This talk makes the case that using US-based cloud services may no longer be a safe or ethical default for organizations operating in Europe. It aims to shift the mindset from convenience to sovereignty, from reliance to control. The talk will consist of five parts: 1. What Is a Data Platform and How Did We Get Here? 2. The Status Quo: What Do Companies Use Today? 3. Why You Should Be Concerned 4. What Are the Alternatives and What Do We Really Need? 5. Call to Action: Join the Movement This talk is targeted at data engineers, architects, and technically-minded decision-makers. No prior knowledge of specific tools is required\u2014just an interest in the future of cloud data infrastructure and a willingness to challenge the status quo."
  },
  "77624": {
    "title": "Counting Groceries with Computer Vision: How Picnic Tracks Inventory Automatically",
    "speakers": [
      "Sven Arends"
    ],
    "type": "Talk",
    "abstract": "In this talk, we'll share how we're using computer vision to automate stock counting, right on the conveyor belt. We'll discuss the challenges we've faced with the hardware, software, and GenAI components, and we'll also review our own benchmark results for the various state-of-the-art models. Finally, we'll cover the practical aspects of GenAI deployment, including prompt optimization, preventing LLM \"yapping,\" and creating a robust feedback loop for continuous improvement.",
    "description_text": "At Picnic, we deliver groceries to customers across three countries. To ensure we deliver what we promise, we rely on precise stock tracking. But with millions of items moving through our highly automated fulfillment center in Utrecht every day, manually counting stock is incredibly labor-intensive. In this talk, we'll share how we're using computer vision to automate stock counting, right on the conveyor belt. From capturing high-quality images of moving products to evaluating and deploying state-of-the-art models, this project challenged us across the stack: hardware, software, and GenAI. We'll explore topics such as: * Why YOLO didn't work for us * Benchmarking the latest (GenAI) vision models * Optimizing prompts with DSPy * Preventing unnecessary LLM \u201cyapping\u201d * Capturing high-quality images of items in motion This session blends practical engineering insights with cutting-edge AI tools making it interesting for anyone applying computer vision to real-world problems."
  },
  "78211": {
    "title": "GenAI governance in practice: patterns, pitfalls & strategies across tools and industries",
    "speakers": [
      "Maarten de Ruiter"
    ],
    "type": "Talk",
    "abstract": "Governing generative AI systems presents unique challenges, particularly for teams dealing with diverse GenAI subdomains and rapidly changing technological landscapes. In this talk, Maarten de Ruiter, Data Scientist at Xomnia, shares practical insights drawn from real-world GenAI use-cases. He will highlight essential governance patterns, address common pitfalls, and provide actionable strategies for teams utilizing both open-source tools and commercial solutions. Attendees will gain concrete recommendations that work in practice, informed by successes (and failures!) across multiple industries",
    "description_text": "This session offers a practical guide to GenAI governance for teams building with modern data tooling and managing complex, unstructured data pipelines. Drawing from Xomnia\u2019s experience across various GenAI subdomains, Maarten will discuss: Frameworks for structuring governance across core GenAI use cases, such as LLM integration, data pipelines, prompt management, and monitoring. - A candid look at tool selection: comparing open-source options and commercial solutions, with suggestions for effective combinations that suit real-world needs. - Techniques for managing unstructured data quality and AI risk, illustrated with code examples, architecture diagrams, and implementation tips. - Application of the \u201cGenAI Governance Maturity Matrix\u201d to assess and elevate your team\u2019s practices, with field-tested recommendations drawn from real client use cases. Expect examples of both what has worked well and what hasn\u2019t, plus the actionable takeaways we wish we\u2019d had from the start. Bring your GenAI governance questions and headaches; the session includes time for troubleshooting and sharing of practical solutions."
  },
  "77594": {
    "title": "Context is King: Evaluating Long Context vs. RAG for Data Grounding",
    "speakers": [
      "Bauke Brenninkmeijer"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "Grounding Large Language Models in your specific data is crucial, but notoriously challenging. Retrieval-Augmented Generation (RAG) is the common pattern, yet practical implementations are often brittle, suffering from poor retrieval, ineffective chunking, and context limitations, leading to inaccurate or irrelevant answers. The emergence of massive context windows (1M+ tokens) seems to offer a simpler path \u2013 just put all your data in the prompt! But does it truly solve the \"needle in a haystack\" problem, or introduce new challenges like prohibitive costs and information getting lost in the middle? This talk dives deep into the engineering realities. We'll dissect common RAG failure modes, explore techniques for building robust RAG systems (advanced retrieval, re-ranking, query transformations), and critically evaluate the practical viability, costs, and limitations of leveraging long context windows for complex data tasks in Python. Leave understanding the real trade-offs to make informed architectural decisions for building reliable, data-grounded GenAI applications.",
    "description_text": "Accurately grounding Large Language Model (LLM) outputs in specific, often private, datasets is crucial for enterprise adoption and reliable data applications. While Retrieval Augmented Generation (RAG) is widely discussed for tackling this \"needle in a haystack\" challenge, practitioners often face significant reliability issues. Concurrently, exponentially growing context windows offer an alternative architectural choice. This talk directly addresses this pivotal decision, moving beyond introductory explanations to provide engineers with a framework for evaluating and implementing *reliable* grounding strategies, acknowledging the common pitfalls of naive approaches. Aimed at Python-proficient Data Engineers, Scientists, AI/ML Engineers, and Researchers building or evaluating LLM systems for specific datasets, this session is especially relevant for those hitting RAG limits or considering large context architectures. We'll briefly explain the RAG pipeline (Load, Split, Embed, Retrieve, Generate) before critically examining *why* it often fails: retrieval irrelevance, suboptimal chunking, context limits, and evaluation hurdles. The talk will then explore Python tools and techniques for improving RAG reliability, such as hybrid search, re-ranking, query transformations, metadata filtering, and adaptive chunking. The session will then dissect the promise and perils of multi-million token contexts. We'll analyze potential benefits like architectural simplicity against drawbacks: API costs, latency, the \"lost in the middle\" issue, potential need for in-context data structuring, and interaction complexity limits. A core segment provides a direct comparative analysis: RAG vs. Long Context across accuracy, cost, latency, scalability, data freshness, implementation complexity, and suitability for different data/tasks. We'll also consider potential hybrid approaches, blending the strengths of both. Comparisons will be supported by data-driven graphs and python code. Attendees will gain a practical decision framework for choosing when to favour robust RAG versus exploring long context models, emphasizing evaluation strategies crucial for both. The talk will summarize key engineering takeaways for building dependable, data-grounded LLM systems. This informative and comparative session aims to equip practitioners with the knowledge to move beyond introductory concepts and make sound architectural decisions for reliable LLM applications using their specific data. **Outline:** - **00:00-00:05 - The Grounding Problem:** Defining the \"needle in a haystack\" challenge for LLMs using specific data. - **00:05-00:20 - Deconstructing RAG & Its Failure Modes:** RAG pipeline; *why it fails* (retrieval, chunking, context, evaluation); Python techniques for *reliable RAG* (hybrid search, re-ranking, query transforms, metadata, adaptive chunking). - **00:20-00:30 - The Long Context Promise & Perils:** Multi-million token contexts; *benefits* (simplicity?) vs. *drawbacks* (cost, latency, \"lost in the middle,\" data structuring, interaction limits). - **00:30-00:40 - Comparative Analysis & Engineering Trade-offs:** RAG vs. Long Context compared on accuracy, cost, latency, scalability, freshness, complexity, data/task fit; potential hybrid approaches. - **00:40-00:45 - Decision Framework & Conclusion:** Choosing RAG vs. Long Context; evaluation strategies; key engineering takeaways for dependable grounded systems."
  },
  "77545": {
    "title": "No labels? No problem! - Hunting Fraudsters with Minimal Labels and Maximum ML",
    "speakers": [
      "Jaap Stefels",
      "Itzel Belderbos"
    ],
    "type": "Talk",
    "abstract": "Card testing is one of the largest growing fraud problems within the payments landscape, with fraudsters launching millions of attempts globally each month. These attacks can cost companies thousands of euros in lost revenue and lead to the distribution of private card details. Detecting this type of fraud is extremely difficult without confirmed labels to train standard supervised ML classifiers. In this talk, we\u2019ll describe how we built a production-ready ML model that now processes hundreds of transactions per second and share the key take-aways from our journey.",
    "description_text": "Card testing is one of the largest growing fraud problems within e-commerce payments: criminals try out slightly modified card numbers tweaking a single digit at the time, to discover valid credentials. This leads to millions of fraudulent transactions per month. Once a card is validated, it can be used to purchase expensive goods or be distributed for further use. Detecting this fraud is extremely difficult as there is no direct feedback that can be used to create labels for a ML model. But how can we train a performant ML model without having a distinction between attacks and legitimate transactions? In this talk, we\u2019ll describe how we tackled the labeling issue which has resulted in a production-ready ML model that now processes hundreds of transactions per second. It's been a classic cat and mouse game: as fraudsters evolve their tactics, we\u2019ve had to stay one step ahead with clever data and modeling strategies. We will share the following lessons and key take-aways to get other ML scientists started with similar unsupervised problems: - Why to choose to apply supervised machine learning to what is fundamentally an unsupervised task - Guidelines for constructing a proxy-labeled dataset, with a use-case example of using time-series and graph-based methods - The key insights we gained from building this model - highlighting both our successes and the challenges we faced Outline - 0-5 min: Introduction and problem statement - 5-8 min: Why we want to use supervised ML for an unsupervised problem - 8-12: Proxy-label trade-offs - 12-25: Our solution to the problem - 25-30: Key take-aways, guidelines and lessons learned"
  },
  "78216": {
    "title": "The Gentle Monorepo: Ship Faster and Collaborate Better",
    "speakers": [
      "Gerben Dekker"
    ],
    "type": "Talk",
    "abstract": "Monorepos promise faster development and smoother cross-team collaboration, but they often seem intimidating, requiring major tooling, buy-in, and process changes. This talk shows how Dexter gradually introduced a Python monorepo by combining a few lightweight tools with a pragmatic, trust-based approach to adoption. The result is that we can effectively reuse components across our various energy forecasting and trade optimization products. We iterate quicker on bringing our research to production, which benefits our customers and supports the renewable energy transition. After this talk, you\u2019ll walk away with a practical blueprint for introducing a monorepo in your context, without requiring heavy up-front work.",
    "description_text": "Many Python product development teams struggle to manage shared code across multiple product repositories. Internal packages seem like a good solution, until releasing and updating them becomes slow, risky, and frustrating. Monorepos offer a compelling alternative, but often come with perceived challenges: steep tooling requirements, inflexible processes, or the need for top-down mandates. At Dexter, we provide our customers with an array of renewable energy forecasting and trade optimization products that share many similar components. To iterate quickly, we need to keep teams autonomous, enabling them to make their own product decisions, but at the same time reuse code and ideas from other teams easily, in cases where it can accelerate progress. As team composition and product-market fit are evolving constantly, we also need our code base to support organizational changes and be quick to onboard to. At the same time, we wanted to avoid a large engineering effort to introduce a heavy-handed monorepo solution. Therefore, we decided to experiment with a monorepo gradually, starting with just two teams. We didn\u2019t rewrite our workflows or adopt heavyweight tools. Instead, we used a small set of simple, open source tools and focused on trust-based coordination and documentation. Today, most of our products are powered by that monorepo, and we\u2019re shipping faster and collaborating more easily. This setup is key for our data science-heavy product suite, as new product features, better forecasting models and smarter trade decision optimizers immediately benefit our customers and support the renewable energy transition. In this talk, I\u2019ll share our playbook: 1. Why we wanted a monorepo The pain of shared packages and coordination overhead The promise of speed and confidence 2. Four essential tools (all simple and open): polylith for structure and modularity uv for fast, reproducible dependency management tach for guarding of package boundaries and selective testing ADRs for lightweight decision making and tracking 3. Organizational tactics: Start with a real use case, instead of a mandate Remain flexible: don\u2019t enforce what doesn\u2019t hurt Document shared norms as they emerge Let success stories pull other teams in 4. Lessons and ongoing challenges: Keeping CI fast and scoped Avoiding \u201cfree rider\u201d effects Making cross-team decisions without blocking progress This talk is for engineers and tech leads in multi-team environments who are interested in monorepos, but hesitant about the cost and coordination. You\u2019ll leave with practical ideas to improve collaboration and speed\u2014without needing to overhaul your entire organization."
  },
  "77058": {
    "title": "Ethics is Not a Feature: Rethinking AI from the Ground Up",
    "speakers": [
      "Dr. Maria B\u00f6rner"
    ],
    "type": "Keynote",
    "abstract": "Ethics is often treated like a product feature\u2014something to be added at the end, polished for compliance, or marketed for trust. But what if that mindset is exactly what\u2019s holding us back? In this keynote, we\u2019ll challenge the idea that ethics is optional or external to the development process. We\u2019ll explore how ethical blind spots in AI systems\u2014from biased models to black-box decisions to unsustainable compute\u2014aren\u2019t just philosophical dilemmas, but human failures with real-world consequences. You\u2019ll learn how to spot ethical risks before they become failures, and discover practical tools and mindsets to build AI that earns trust\u2014without compromising on innovation. From responsible data practices to transparency techniques and green AI strategies, we\u2019ll connect the dots between values and code. This isn\u2019t just a lecture\u2014it\u2019s a call to rethink how we build the future of AI\u2014together.",
    "description_text": "."
  },
  "77053": {
    "title": "Closing notes",
    "speakers": [],
    "type": "Closing Notes",
    "abstract": "Closing notes",
    "description_text": "Closing notes"
  },
  "77601": {
    "title": "Large-Scale Video Intelligence",
    "speakers": [
      "Irene Donato",
      "Antonino Ingargiola"
    ],
    "type": "Talk",
    "abstract": "The explosion of video data demands search beyond simple metadata. How do we find specific visual moments, actions, or faces within petabytes of footage? This talk dives into architecting a robust, scalable multi-modal video search system. We will explore an architecture combining efficient batch preprocessing for feature extraction (including person detection, face/CLIP-style embeddings) with optimized vector database indexing. Attendees will learn practical strategies for managing massive datasets, optimizing ML inference (e.g., lightweight models, specialized runtimes), and bridging pre-computed indexes with real-time analysis for deeper insights. This session is for data scientists, ML engineers, and architects looking to build sophisticated video understanding capabilities. Audience: Data Scientists, Machine Learning Engineers, Data Engineers, System Architects. Takeaway: Attendees will learn architectural patterns and practical techniques for building scalable multi-modal video search systems, including feature extraction, vector database utilization, and ML pipeline optimization. Background Knowledge: Familiarity with Python, core machine learning concepts (e.g., embeddings, classification), and general data processing pipelines is beneficial. Experience with video processing or computer vision is a plus but not strictly required.",
    "description_text": "Searching vast video archives (petabytes) requires deep visual and temporal understanding, far beyond metadata. This talk details our journey building a system for large-scale, multi-modal video retrieval, empowering complex analytical queries. We will focus on generalizable techniques, particularly emphasizing efficient and adaptable model usage to overcome the inherent challenges of video data."
  },
  "77754": {
    "title": "Potato breeding using image analysis in a production setting",
    "speakers": [
      "Dick Abma",
      "Rik Nuijten"
    ],
    "type": "Talk",
    "abstract": "The scale-up company Solynta focuses on hybrid potato breeding, which helps achieve improvements in yield, disease resistance, and climate adaptation. Scientific innovation is part of our core business. Plant selections are highly data-driven, involving, for example, drone observations and genetic data. Minimal time-to-production for new ideas is essential, which is facilitated by our custom AWS devops platform. This platform focusses on automation and accessible data storage. In this talk, we introduce how computer vision (YOLO and SAM modelling) enables monitoring traits of plants in the field, and how we operate these models. This further entails: \u2022 Our experience from training and evaluating models on drone images \u2022 Trade-offs selecting AWS services, Terraform modules and Python packages for automation and robustness \u2022 Our team setup that allows IT specialists and biologists to work together effectively The talk will provide practical insights for both data scientists and DevOps engineers. The main takeaways are that object detection and segmentation from drone maps, at scale, are achievable for a small team. Furthermore, with the right approach, you can standardise a DevOps platform to let operations and developers work together.",
    "description_text": "Outline (30 min): \u2022 Introduction to hybrid potato breeding (3 min) \u2022 Challenges/solutions in breeding (5 min) \u2022 YOLO and SAM modelling using drone data (10 min) \u2022 Working together on a DevOps platform (10 min) \u2022 Summary (2 min)"
  },
  "77925": {
    "title": "Formula 1 goes Bayesian: Time Series Decomposition with PyMC",
    "speakers": [
      "Wesley Boelrijk"
    ],
    "type": "Talk",
    "abstract": "Forecasting time series can be messy, data is often missing, noisy, or full of structural changes like holidays, outliers, or evolving patterns. This talk shows how to build interpretable time series decomposition models using PyMC, a modern probabilistic programming library. We\u2019ll break time series into trend, seasonality, and noise components using engineered time features (e.g., Fourier and Radial Basis Functions). You\u2019ll also learn how to model correlated series using hierarchical priors, letting multiple time series \"learn from each other.\" As a case study, we\u2019ll analyze Formula 1 lap time data to compare drivers and explore performance consistency using Bayesian posteriors. This is a hands-on, code-first talk for data scientists, ML engineers, and researchers curious about Bayesian modeling (or Formula 1). Familiarity with Python and basic statistics is helpful, but no deep knowledge of Bayes is required.",
    "description_text": "We\u2019ll build decomposition models from scratch using PyMC, focusing on: * 0\u20135 min: Introducing time series decomposition and when you might need it * 5\u201310 min: Building a baseline model (trend + noise) * 10\u201315 min: Adding seasonal components and outlier indicators * 15\u201320 min: Modeling correlated time series with hierarchical priors * 20\u201325 min: Using posteriors for forecasting and anomaly detection * 25\u201330 min: Wrap up and comparison with ARIMA/Prophet This session prioritizes practical modeling over theory. Expect live coding on F1 data and takeaways ready to apply to your own time series projects."
  },
  "77653": {
    "title": "Designing tests for ML libraries \u2013 lessons from the wild",
    "speakers": [
      "Sayak Paul",
      "Benjamin Bossan"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "In this talk, we will cover how to write effective test cases for machine learning (ML) libraries that are used by hundreds of thousands of users on a regular basis. Tests, despite their well-established need for trust and foolproofing, often get less prioritized. Later, this can wreak havoc on massive codebases, with a high likelihood of introducing breaking changes and other unpleasant situations. This talk deals with our approach to testing our ML libraries, which serve a wide user base. We will cover a wide variety of topics, including the mindset and the necessity of minimal-yet-sufficient testing, all the way up to sharing some practical examples of end-to-end test suites.",
    "description_text": "* Why revisit an established topic? * How do ML libraries differ from regular Python libraries and how does it impact their testing? * Types of ML Libraries * Platform-level libraries (PyTorch, JAX, etc.) * Modeling libraries (Transformers, Diffusers, etc.) * Utility libraries (PEFT, TorchAO, etc.) * Data-related libraries (Torchvision, Datasets) * Briefing about how testing and CI are approached for the modeling and utility libraries at HF * Best practices from the wild * Python version coverage \u2013 are we covering all Python versions, or is there a minimum requirement? * Operating system distribution coverage \u2013 are we only targeting Linux? * Should code coverage be approached in the same way it\u2019s approached for regular software? * Benchmarking tests \u2013 Do model forward passes take the same amount of time in a new feature? If there\u2019s an increase, can we explain it? * Conditional accelerator tests (for certain changes trigger GPU tests, for example) * Approaching regression tests \u2013 with each new version of the library, outputs shouldn\u2019t change without a plausible justification * Dealing with known failures \u2013 should we test known failures? By the end of this talk, the audience will have a good understanding of the effective approaches to support modern ML libraries."
  },
  "77759": {
    "title": "What Works: Practical Lessons in Applying Privacy-Enhancing Technologies (PET) in Data Science",
    "speakers": [
      "Yuliya Sapega",
      "Joanna Pasiarska"
    ],
    "type": "Talk",
    "abstract": "Privacy-Enhancing Technologies (PETs) promise to bridge the gap between data utility and privacy \u2014 but how do they perform in practice? In this talk, we\u2019ll share real-world insights from our hands-on experience testing and implementing leading PET solutions across various data science use cases. We explored tools such as differential privacy libraries, homomorphic encryption frameworks, federated learning, multi-party computation, etc. Some lived up to their promise \u2014 others revealed critical limitations. You\u2019ll walk away with a clear understanding of which PET solutions work best for which types of data and analysis, what trade-offs to expect, and how to set realistic goals when integrating PETs into your workflows. This session is ideal for data professionals and decision-makers who are navigating privacy risks while still wanting to innovate responsibly.",
    "description_text": "In a world where data-driven innovation often clashes with growing privacy demands, Privacy-Enhancing Technologies (PETs) are gaining momentum. But theory and whitepapers are one thing \u2014 practical implementation is another. In this 30-minute talk, we take you behind the scenes of real-life data science projects where PETs were tested and applied. You'll hear honest, experience-based insights about what worked, what didn\u2019t, and why. We\u2019ll cover widely known approaches like differential privacy, homomorphic encryption, federated learning, multi-party computation\u2014 and show where each one fits (or doesn\u2019t) in real-world settings. Whether you\u2019re working in healthcare, government, HR, or finance, you\u2019ll learn: \u2022 How to match PET tools with specific use cases \u2022 Where the trade-offs lie in utility, scalability, and compliance \u2022 What technical and organizational conditions are needed for success This session is fast-paced, practical, and designed for anyone balancing innovation with responsible data use \u2014 from data scientists and engineers to compliance leads and product managers."
  },
  "79196": {
    "title": "Measure twice, deploy once: Evaluation of retrieval systems",
    "speakers": [
      "Paul verhaar",
      "Marten koopmans"
    ],
    "type": "Talk",
    "abstract": "Improving retrieval systems\u2014especially in RAG pipelines\u2014requires a clear understanding of what\u2019s working and what isn\u2019t. The only scalable way to do that is through meaningful metrics. In this talk, we share insights from building a platform-agnostic search and retrieval product, and how we balance performance against cost. Bigger models often give better results\u2026 but at what price? We explain how to assess what\u2019s \u201cgood enough\u201d and why the choice of benchmark really matters.",
    "description_text": "We\u2019ll dive into the metrics-based evaluation of retrieval systems for real-world RAG applications, covering both open- and closed-source models. Expect practical takeaways on managing trade-offs between model quality and cost, and how to build evaluation pipelines that reflect production needs."
  },
  "77553": {
    "title": "Uncertainty Unleashed: Wrapping Your Predictions in Honesty with Conformal Prediction",
    "speakers": [
      "Konstantinos Tsoumas"
    ],
    "type": "Talk",
    "abstract": "There are a lot of models working in production as you're reading this. Lots of them are giving uncalibrated outputs without being explicit on how much one can trust the result. Especially when it comes to imbalanced datasets. More so, relying on biased estimates can lead to overly aggressive decisions. In this hands\u2011on talk, we\u2019ll demystify conformal methods using MNIST\u2014the world\u2019s favorite handwritten\u2011digit playground (to make the talk more fun & interactive)- with two goals in mind: explain & prove what an unbiased guarantee is and how it can be calculated but also why should you care and why does it matter so much. Attendees may leave equipped with: uncertainty guarantee understanding in classification, identify common pitfalls that lead to biased uncertainty estimates, how to apply it (even in difficult contexts like imbalanced datasets - an example will be given).",
    "description_text": "This talk is centered around estimating uncertainty with unbiased guarantees, especially in imbalanced dataset scenarios. The method proposed to approach this problem is Conformal Prediction. Conventional classifiers give point predictions but rarely communicate their confidence, often leading to overconfident decisions (especially in high-stake domains like Medicine). Conformal prediction fills this gap by producing prediction sets that contain the true label with a user\u2013specified probability (e.g., 90%) without assuming any particular data distribution. Imbalanced datasets make the above a huge hurdle and SMOTE is not of any help. The proposed talk structure is (but not limited to): - What do we mean with guarantees and why is it important? (MNIST example) - How the hall can we get out of this? - Imbalanced datasets are even more difficult to be dealt with. What now? - As a user, how can you estimate unbiased uncertainty guarantees? - Remember, Conformal Prediction is a recipe and there are always different ways or ingredients that may fit a use-case better (quick overview over classification focused CP alternatives). Target audience: Curious people. They may call themselves Practicing Data Scientists, ML Engineers or researchers at times but basic ML understanding would be sufficient to keep up with the talk. No PhD in Stats required <- guaranteed. This talk is unique among all prior PyData conformal prediction sessions\u2014unlike the time\u2011series focus at PyData Seattle 2023, the large\u2011scale forecasting angle at PyData London 2024, the energy\u2011grid case study at PyData Eindhoven 2023, the MAPIE library deep\u2011dive at PyData Global 2024, the gentle intro in Amsterdam 2024, the sktime/skpro probabilistic workshop in Amsterdam 2023, the regression\u2011only focus in London 2019 PyData- as is the first to deliver provably unbiased uncertainty guarantees for general classifiers on a multi\u2011class playground like MNIST and (more importantly so) in imbalanced datasets."
  },
  "77557": {
    "title": "Causal Inference Framework for incrementality : A Case Study at Booking to estimate incremental CLV due to App installs",
    "speakers": [
      "Netesh",
      "Nazl\u0131 Alag\u00f6z"
    ],
    "type": "Talk",
    "abstract": "This talk dives into the challenge of measuring the causal impact of app installs on customer loyalty and value, a question at the heart of data-driven marketing. While randomized controlled trials are the gold standard, they\u2019re rarely feasible in this context. Instead, we\u2019ll explore how observational causal inference methods can be thoughtfully applied to estimate incremental value with careful consideration of confounding, selection, and measurement biases. This session is designed for data scientists, marketing analysts, and applied researchers with a working knowledge of statistics and causal inference concepts. We\u2019ll keep the tone practical and informative, focusing on real-world challenges and solutions rather than heavy mathematical derivations. Attendees will learn: * How to design robust observational studies for business impact * Strategies for covariate selection and bias mitigation * The use of multiple statistical and design-based causal inference approaches * Methods for validating and refuting causal claims in the absence of true randomization We\u2019ll share actionable insights, code snippets, and a GitHub repository with example workflows so you can apply these techniques in your own organization. By the end of the talk, you\u2019ll be equipped to design more transparent and credible causal studies-and make better decisions about where to invest your marketing dollars. Requirements: A basic understanding of causal inference and Python is recommended. Materials and relevant links will be shared during the session",
    "description_text": "For most companies one of the top marketing priorities is to increase the Customer Loyalty towards their brand and products and therefore increase the lifetime value from their customers. One of the ways that companies try to achieve this is to onboard customers onto their app. While mobile App users tend to have higher engagement and retention rates, often generating more profits through direct purchases, the challenge lies in determining if this is due to app installs or if high value customers are more likely to install the app. The ideal way to measure the causal impact of an app install would be to set up a randomized controlled trial, but this is impractical as we cannot force users into treatment (install the app) and control (do not install) arm. Therefore, we leveraged observational causal inference methods to understand the incremental value of an app install, where we compared app users to non app users. The biggest challenge of using observational causal inference for business decision making is to demonstrate stronger evidence for causal conclusions and address biases from confounding, selection and measurement, which can result in an underestimate or overestimate of the effect of interest. Our work aims to address these concerns by exploring covariate selection strategies, different statistical approaches, refutation methods and design-based approaches. The estimated causal effects from this study helped the business to identify the most valuable install acquisition channels and optimize the app marketing spend to drive more incremental CLV. This work can be considered part of wider efforts to improve the transparency and robustness of observational causal inference studies by thoughtful application of multiple approaches (statistical and design based) each with their own strengths and weaknesses. Our next step is to implement a comprehensive sensitivity analysis to further strengthen our causal claims."
  },
  "77417": {
    "title": "Flip the Plan: Fast-Track Your AI/ML Model Integration with a Back-to-Front Implementation Strategy",
    "speakers": [
      "Florenz Hollebrandse"
    ],
    "type": "Talk",
    "abstract": "\"How quickly will you be able to get this model into production?\" is a common question in analytical projects. Often, this is the first time anyone considers the complexities of deploying models within enterprise systems. This talk introduces an approach to enhance the success rate of complex AI/ML integration projects while reducing time-to-market. Using examples from global banks J.P. Morgan and ING, we will demonstrate team organisation and engineering patterns to achieve this. This talk is ideal for data scientists, engineers, and product managers interested in adopting an efficient Model Development Lifecycle (MDLC).",
    "description_text": "**Part 1: Breaking Down Barriers in Your AI/ML Projects (10 mins)** - **Context Setting**: Discuss common reasons why AI/ML projects fail to progress beyond the pilot phase, supported by relevant research and statistics. - **Engineering Integration Challenges**: Highlight the late consideration of engineering integration/inference mode as a key failure reason. - **Waterfall Model Issues**: Explain the \"waterfall\" model of data science vs engineering, emphasising the communication overhead it creates. - **Conway's Law**: Use Conway's law to illustrate how team structures impact project success rates and time-to-market. - **Ideal Team Structure**: Propose an integrated, collaborative, iterative team structure. Show an animation illustrating how concurrent phases can reduce time-to-market by 50%. - **Audience Poll**: Gauge the feasibility of the ideal team structure (Yes/No) and discuss reasons for responses. **Part 2: Implementing the Back-to-Front Model Deployment Pattern (20 mins)** - **Efficient Deployment Architecture**: Introduce the back-to-front model deployment pattern as the main takeaway. - **Real-World Examples**: Provide detailed examples from J.P. Morgan (credit risk alerting system) and ING (Customer Due Diligence automation). - **Key Concepts**: Explain starting from model inference, defining data contracts, considering failure modes, orchestrating model training, and wiring up ETL/data pipelines. **[Suitable for 45 min extended talk] Detailed/hands-on walk-through**: Present the open-source project Inference Server (https://github.com/jpmorganchase/inference-server), which supports the back-to-front deployment pattern by deploying an \"empty\" model and progressively \u201cback-filling\u201d actual model logic."
  },
  "77689": {
    "title": "Streamlining data pipeline development with Ordeq",
    "speakers": [
      "Niels Neerhoff",
      "Simon Brugman"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "In this talk, we will introduce Ordeq, a cutting-edge data pipeline development framework used by data engineers, scientists and analysts across ING. Ordeq helps you modularise pipeline logic and abstract IO, elevating projects from proof-of-concepts to maintainable production-level applications. We will demonstrate how Ordeq integrates seamlessly with popular data processing tools like Spark, Polars, Matplotlib, DSPy, and orchestration tools such as Airflow. Additionally, we showcase how you can leverage Ordeq on public cloud offering like GCP. Ordeq has 0 dependencies and is available under MIT license.",
    "description_text": "The talk is targeted at data engineers, scientists, analysts and machine learning engineers who would like to kick-start their next data pipelines project. Our talk will: - Introduce Ordeq, its key motivations, and design philosophy - Compare Ordeq against existing tools like dbt, Hamitlon, Kedro and LangChain - Conduct a technical deep-dive into the core components of Ordeq - Showcase Ordeqs seamless integration with popular data processing tools like Spark, Polars, Matplotlib and DSPy - Demonstrate Ordeq in several case studies - Conclude with a Q&A session"
  },
  "77634": {
    "title": "Continuous monitoring of model drift in the financial sector",
    "speakers": [
      "Denis Gaitan",
      "Agustin Iniguez"
    ],
    "type": "Talk",
    "abstract": "In today\u2019s financial sector, the continuous accuracy and reliability of machine learning models are crucial for operational efficiency and effective risk management. With the rise of MLOps (Machine Learning Operations), automating monitoring mechanisms has become essential to ensure model performance and compliance with regulations. This presentation introduces a method for continuous monitoring of model drift, highlighting the benefits of automation within the MLOps framework. This topic is particularly interesting because it addresses a common challenge in maintaining model performance over time and demonstrates a practical solution that has been successfully implemented in the bank. This talk is aimed at data scientists, machine learning engineers, and MLOps practitioners who are interested in automating the monitoring of machine learning models. Attendees will be guided on how to continuous monitor model drift within the MLOps framework. They will understand the benefits of automation in this context, and gain insights into MLOps best practices. A basic understanding of MLOps principles, and statistical techniques for model evaluation will be helpful but not strictly needed. The presentation will be an informative talk with a focus on the design and implementation. It will include some mathematical concepts but will primarily be demonstrating real-world applications and best practices. At the end we encourage you to actively monitor model drift and automate your monitoring processes to enhance model accuracy, scalability, and compliance in your organizations.",
    "description_text": "In this session, we will talk about the integration of automated model drift detection within the MLOps framework, focusing on best practices and practical implementation. Our goal is to demonstrate how continuous monitoring can enhance the accuracy, scalability, and compliance of machine learning models in the financial sector. We will begin by introducing the concept of MLOps (Machine Learning Operations) and its best practices, highlighting the importance of monitoring machine learning models, with a particular emphasis on detecting and managing model drift [5 minutes]. Next, we will outline the key requirements for effective model drift monitoring needed to ensure that models remain accurate and reliable over time [2 minutes]. We will then introduce the specific machine learning model that will be subjected to our monitoring solution, providing context on the model's purpose and its significance within the bank [2 minutes]. Following the model introduction, we will explain the framework we created to develop our monitoring solution, covering the design principles, architecture, and the rationale behind our approach [5 minutes]. In this section, we will present the implementation of our monitoring solution, demonstrating how we integrated it into our CI/CD pipeline [4 minutes], developed the necessary code, and deployed it on Databricks. This will include a step-by-step walkthrough of the technical setup [4 minutes]. We will showcase the final dashboard that presents the results of our monitoring solution, highlighting key metrics and insights derived from continuous model monitoring [4 minutes]. Finally, we will discuss the initial results and the valuable learnings we gained from implementing our monitoring solution [3 minutes]."
  },
  "80067": {
    "title": "Quiet on Set: Building an On-Air Sign with Open Source Technologies",
    "speakers": [
      "Danica Fine"
    ],
    "type": "Talk",
    "abstract": "Using a Raspberry Pi and a powerful trio of open-source technologies\u2014Apache Kafka, Apache Flink, and Apache Iceberg\u2014learn how to build a custom on-air sign to signal when you're on a call and discover how this same scaffolding can be scaled for millions of users.",
    "description_text": "- While many of us have adapted to work from home life, one major problem remains: finding an easy way to keep folks in your home away from your workspace when you\u2019re on an important call. Dust off your Raspberry Pi\u2013\u2013let\u2019s build a custom on-air sign with Apache Kafka\u00ae, Apache Flink\u00ae, and Apache Iceberg\u2122! - We\u2019ll begin by writing Python scripts to capture key events\u2013\u2013such as when a Zoom meeting is running and when a camera is being used\u2013\u2013and produce it into Kafka. The live data are then consumed by a Raspberry Pi script to drive the operation of a custom designed on-air sign. From there, you\u2019ll be introduced to the ins and outs of FlinkSQL for stream processing as we wrangle the data into a better format for downstream use. And, finally, we\u2019ll see Iceberg in action and learn how to use query engines to analyze meeting and recording trends. - By the end of the session, you\u2019ll be well-acquainted with this powerful trio of open source technologies and know how you could use the same scaffolding and scale out a simple, at-home project to millions of users and simultaneous events."
  },
  "81716": {
    "title": "Diversity Isn\u2019t a Buzzword, It\u2019s a Business Case",
    "speakers": [
      "Ba\u015fak Eskili"
    ],
    "type": "Talk",
    "abstract": "Thirteen years ago, I walked into my first programming class as one of a few women among seventy men and the numbers haven\u2019t shifted much despite all the bootcamps and \u201cGirls Who Code\u201d posters promised. Today, women still make up under 30% of the tech workforce and even fewer in leadership. This talk blends personal experience and data to explore why that matters: companies with more women in leadership perform better, women are built to thrive under pressure, and when women are missing from tech, their perspectives are missing from the data too. Diversity isn\u2019t charity or PR, it's how we build better systems.",
    "description_text": "Thirteen years ago, I walked into my first lecture as a computer science student, a classroom of seventy men, with only a few women among us. That classroom was my entry point into tech and it turns out, the ratios haven\u2019t changed much. Women still make up under 30% of the tech workforce, and even fewer in leadership. This talk connects that lived experience to the bigger picture: how the absence of women shapes the systems we build. Companies with more women in senior leadership see stronger culture, higher engagement, and even higher profits. Biologically, women are wired to handle stress, countering the myth that they\u2019re less suited for high-pressure leadership. And when half the world is missing from the teams designing our products, their perspectives are missing from the data which means the systems we build are biased by design. I\u2019ll share what it takes to stay in a field that keeps quietly asking if you belong and why I stayed anyway. Because diversity doesn\u2019t just change who\u2019s at the table; it changes the quality of what we build."
  },
  "77132": {
    "title": "Actionable Techniques for Finding Performance Regressions",
    "speakers": [
      "Thijs Nieuwdorp",
      "Jeroen Janssens"
    ],
    "type": "Talk",
    "abstract": "Ever been burned by a mysterious slowdown in your data pipeline? In this session, we'll reveal how a stealthy performance regression in the Polars DataFrame library was hunted down and squashed. Using git bisect, Bash scripting, and uv, we automated commit compilation and benchmarking across two repos to pinpoint a commit that degraded multi-file Parquet loading. This led to challenging assumptions and rethinking performance monitoring for the Python data science library Polars.",
    "description_text": "Performance regressions can sneak in and sabotage your workflow before you even notice, until it\u2019s too late. In this talk, we\u2019ll take you on a step-by-step journey through the rigorous debugging process that uncovered a hidden performance bug in Polars. By combining git bisect with Bash scripts and uv for automated benchmarking, we systematically isolated the offending commit that was dragging down multi-file Parquet performance. This unfiltered, pragmatic approach didn\u2019t just fix the issue. It sparked a shift in how the Polars team monitors performance, paving the way for continuous evaluation with prebuilt binaries. Key takeaways include: - Systematic Debugging: Learn how to use git bisect to narrow down performance issues with surgical precision. - Automation in Action: See how Bash scripting and uv can automate the compilation and benchmarking process, saving you time and head-scratching. - Data-Driven Decisions: Discover practical methods to analyze benchmark results and inform your performance optimization strategies. - Continuous Monitoring: Understand why integrating ongoing performance checks into your development workflow is not just a nice-to-have, but essential. If you\u2019re a Python developer or data scientist tired of vague performance monitoring and ready for a no-nonsense, forward-thinking approach, this session is for you. Expect honest insights, a healthy dose of skepticism, and actionable techniques to ensure your projects run as efficiently as they should."
  },
  "79199": {
    "title": "From pixel to predictions: A journey through our CT image pipeline in pig breeding using POSIT",
    "speakers": [
      "Lisette van der Zande"
    ],
    "type": "Talk",
    "abstract": "How do you turn a CT scan of a pig into usable data for large-scale genetic research? At Topigs Norsvin, we scan 10,000 male pigs each year using high-resolution CT imaging. This allows us to look inside the animals and assess carcass quality, muscle composition, and indicators of health. We use this data to inform selection decisions and improve the accuracy of our breeding program. In this talk, I'll walk you through the journey of CT data: from scan acquisition and processing to how we extract traits and integrate them into the breeding program. A key part of this process is POSIT, a lightweight project structure that helps us manage complexity, ensure reproducibility and scale our pipelines effectively. While the biological context is specific, the data challenges are familiar to any data professional.",
    "description_text": "Topigs Norsvin is a global leader in genetics research for pig populations, applying data science to drive sustainable and responsible breeding practices. The company is farmer-owned, with roots in the Netherlands and Norway, and operates in over 50 countries worldwide. We are fully committed to contributing to responsible and sustainable pig production for future generations. Sustainability is a main driver for our breeding goals, research, and support. The research team also plays a vital role in developing innovative solutions for producers, processors, and retailers. Sustainability, better animal welfare, and efficient production are core elements in these solutions. We believe that with these elements, we can contribute to the future of pork production and continue to help our customers maximize the potential of our products."
  },
  "81564": {
    "title": "Leading through the GenAI hype cycle: the good, the bad, and the ugly",
    "speakers": [
      "Omar Hommos",
      "Judith Redi",
      "Ninghang Hu"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "Leaders operate across three dimensions: people, business, and technology. A generational shockwave like GenAI has large-scale and fast impact (be it true or perceived impact) on these three dimensions. We leaders then face a sprint of interesting challenges like: - How to determine what value of this technology is currently underestimated vs overestimated, and how does this change in the future? - How do we contribute to the larger leadership team across different skillsets (sales, product, etc) in the company, being the subject matter experts on this topic? - How do we steer through the learning curve, for both the individual contributors in the team, and the wider company? And few more similar challenges! Join us for a nice panel discussion on this topic.",
    "description_text": "."
  },
  "79254": {
    "title": "Microlog: Explain Your Python Applications with Logs, Graphs, and AI",
    "speakers": [
      "Chris Laffra"
    ],
    "type": "Talk",
    "abstract": "Microlog is a lightweight continuous profiler and logger for Python that helps developers understand their applications through interactive visualizations and AI-powered insights. With extremely low overhead and a 100% Python stack, it makes it easy to trace performance issues, debug unexpected behavior, and gain visibility into production systems.",
    "description_text": "Modern Python applications are often opaque, especially in production. Debugging performance issues, understanding complex workflows, and tracing unexpected behavior can feel like navigating in the dark. Traditional logging solutions and profilers are either too invasive, too noisy, or too hard to interpret. Microlog is a lightweight continuous profiler and logger designed to make Python application behavior visible, explainable, and actionable. With interactive in-browser visualizations and AI-assisted insights, Microlog helps developers and data practitioners diagnose performance problems and understand their applications like never before \u2014 all with minimal overhead and minimal external dependencies. In this talk, we will introduce Microlog\u2019s design and philosophy, walk through live examples, and demonstrate how it can be used to identify hidden issues in production, debug faster, and ultimately write better software. Whether you're maintaining a large service, building ML pipelines, or just tired of guessing what your code is doing \u2014 this tool was built for you. Microlog is an open-source project: https://github.com/micrologai/microlog"
  },
  "77028": {
    "title": "Image processing, artificial intelligence, and autonomous systems",
    "speakers": [
      "Judith Dijk"
    ],
    "type": "Keynote",
    "abstract": "In this talk, an overview of the field of image processing and the impact of artificial intelligence on this field are shown. Starting from the different tasks that can be performed with image processing, solutions using different AI technologies are shown, including the use of generative AI. Finally, the effect of AI for autonomous systems, and the challenges that are faced are discussed.",
    "description_text": "The field of image processing and the influence of artificial intelligence (AI) on its evolution will be explored. We will begin by outlining the various tasks that image processing can accomplish, from image enhancements to complex information extraction. The discussion will then delve into how different AI technologies, including generative AI, are revolutionizing these tasks, providing innovative solutions and unprecedented capabilities. Furthermore, we will discuss the topic of AI in autonomous systems, highlighting its transformative effects and the new possibilities it unlocks. The talk also addresses some of the challenges for autonomous systems, offering insights into ongoing research and future directions."
  },
  "77596": {
    "title": "Untitled13.ipynb",
    "speakers": [
      "Vincent Warmerdam"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "For well over a decade, Python notebooks revolutionized our field. They gave us so much creative freedom and dramatically lowered the entry barrier for newcomers. Yet despite all this ... it has been a decade! And the notebook is still in roughly the same form factor. So what if we allow ourselves to rethink notebooks ... really rethink it! What features might we come up with? Can we make the notebook understand datasources? What about LLMs? Can we generate widgets on the fly? What if we make changes to Python itself? This presentation will be a stream of demos that help paint a picture of what the future might hold. I will share my latest work in the anywidget/marimo ecosystem as well as some new hardware integrations. The main theme that I will work towards: if you want better notebooks, reactive Python might very well be the future.",
    "description_text": "There are going to be many demos in this talk that will highlight what happens if we force Python to behave reactively instead of imperatively. - Some of these demos will show notebooks widgets that help you play with your data by merging javascript/Python interaction. - Others will focus on how notebooks can be made self reproducible by supporting UV/pytest directly in the notebook itself. - But we will also explore how LLMs fit into the bigger picture, especially when you're interested to work with data. Want to zero-shot an interactive notebook around your data? Then it helps if the datasources automatically provide schemas. - Oh, and another fun thing, thanks to WASM you technically won't even need a local Python environment anymore for any of the things that I am talking about. I will demonstrate that too!"
  },
  "77834": {
    "title": "Scaling Trust: A practical guide on evaluating LLMs and Agents",
    "speakers": [
      "George Chouliaras",
      "Antonio Castelli"
    ],
    "type": "Talk",
    "abstract": "Recently, the integration of Generative AI (GenAI) technologies into both our personal and professional lives has surged. In most organizations, the deployment of GenAI applications is on the rise, and this trend is expected to continue in the foreseeable future. Evaluating GenAI systems presents unique challenges not present in traditional ML. The main peculiarity is the absence of ground truth for textual metrics such as: text clarity, location extraction accuracy, factual accuracy and so on. Nevertheless the non-negligible model serving cost demands an even more thorough evaluation of the system to be deployed in production. Defining the metric ground truth is a costly and time consuming process requiring human annotation. To address this, we are going to present how to evaluate LLM-based applications by leveraging LLMs themselves as evaluators. Moreover we are going to outline the complexities and evaluation methods for LLM-based Agents which operate with autonomy and present further evaluation challenges. Lastly, we will explore the critical role of evaluation in the GenAI lifecycle and outline the steps taken to integrate these processes seamlessly. Whether you are an AI practitioner, user or enthusiast, join us to gain insights into the future of GenAI evaluation and its impact on enhancing application performance.",
    "description_text": "The accelerating integration of LLMs and autonomous Agents into industry applications highlights an urgent need for robust evaluation methodologies, a step that is currently often bypassed or inadequately performed. This talk directly addresses this gap by providing a comprehensive guide to why and how to evaluate both single LLM outputs and LLM-driven Agent behaviors. We will delve into the nuances that make GenAI evaluation distinct, present the \"LLM-as-a-judge\" methodology for single LLMs, and extend these principles to the multifaceted evaluation of LLM-based Agents, covering key metrics and operational complexities. Crucially, we will showcase how to implement these evaluations using accessible open-source frameworks. This session is designed for a diverse audience including GenAI practitioners, product managers, and technical leaders shaping GenAI strategy, alongside enthusiasts keen to understand how these powerful models can be reliably assessed. A basic familiarity with GenAI technologies and a high-level understanding of how single LLMs and LLM-based agents function will be beneficial. An outline of the talk is as follows: - The GenAI Evaluation Imperative: Why traditional metrics fall short and the risks of inadequate GenAI evaluation. - The Uniqueness of GenAI Models: What makes LLM and Agent evaluation fundamentally different? - LLM-as-a-Judge for Single LLMs: - Theory: Principles of using an LLM to evaluate text quality - Practice: Demonstrating evaluation of LLMs using open-source frameworks - Evaluating LLM-Based Autonomous Agents: - Methodology: Defining success for Agents, assessing tool use, task completion, consistency - Key Metrics & Challenges: Beyond single responses \u2013 evaluating multi-turn interactions and autonomy - Open-source tools for Agent evaluation Embedding Evaluation in the GenAI Lifecycle: From development and testing to continuous monitoring in production. - Conclusion & Key Takeaways By the end of this talk, attendees will possess a solid understanding of why GenAI evaluation is critical, be equipped with established methods for evaluating both single LLMs and LLM-based agents using the LLM-as-a-judge paradigm, and know which open-source frameworks can facilitate these processes. This knowledge will empower them to build more reliable, trustworthy, and effective GenAI applications."
  },
  "77536": {
    "title": "How to Keep Your LLM Chatbots Real: A Metrics Survival Guide",
    "speakers": [
      "Maria Bader"
    ],
    "type": "Talk",
    "abstract": "In this brave new world of vibe coding and YOLO-to-prod mentality, let\u2019s take a step back and keep things grounded (pun intended). None of us would ever deploy a classical ML model to production without clearly defined metrics and proper evaluation, so let's talk about methodologies for measuring performance of LLM-powered chatbots. Think of retriever recall, answer relevancy, correctness, faithfulness and hallucination rates. With the wild west of metric standards still in full swing, I\u2019ll guide you through the challenges of curating a synthetic test set, and selecting suitable metrics and open-source packages that help evaluating your use case. Everything is possible, from simple LLM-as-a-judge approaches like those inherent to many packages like MLFLow now up to complex multi-step quantification approaches with Ragas. If you work in the GenAI space or with LLM-powered chatbots, this session is for you! Prior or background knowledge is of advantage, but not required.",
    "description_text": "[min 0-4] Introduction: A brief overview of common LLM-powered chatbots (RAG, agentic). Focus will be on a high-level structure, highlighting the sections we\u2019ll want to evaluate later (retriever, QA-chain) [min 4-5] Motivation: To optimise performance we first need to identify the performance of the individual elements. We need a standardized measure to understand if optimising leads to measurable improvements. [min 5-7] Step 1, the evaluation dataset: Curating a suitable test set is key for comprehensive metrics. Manually creating a test set is tedious, Ragas offers a set of tools that allow to generate a synthetic test set. I will give an overview over the tools for test set generation and show examples. The examples will be later used with the evaluation metrics. [min 7-20] Step 2, metrics for RAG evaluation: I will showcase with examples a set of metrics that allows for a fast comparison and covers the complete RAG pipeline. The goal is not to be exhaustive, but rather give the attendees a set of metrics that have been shown to be useful in combination with each other. This covers: - Metrics to evaluate the document retriever performance: Context Precision (proportion of retrieved documents that are relevant to the query) and Context Recall (number of relevant documents were successfully retrieved) - QA-Metrics: Evaluate the chatbot response with respect to the user question in terms of Answer Relevancy and with respect to the ground truth in terms of Answer Correctness, as well as Faithfulness that evaluates answer with respect to context. Open-source packages: For each of the metrics above, we\u2019ll compare how various packages (MLFlow, DeepEval, Ragas) calculate these metrics. - Custom metric Hallucination Rate that measures the likelihood of hallucination [min 20-25] Agentic and other metrics: Ragas also offers possibilities to evaluate the tool workflow of agentic frameworks, e.g. through Tool Call Accuracy, and additional general purpose metrics like the Summarization Score. The goal here is to give a short impression of these additional metrics that the package offers [min 25-30] Conclusion: As a conclusion I offer tips and tricks on how to select the best metric for your use case. The goal is that the attendees walk away with some concrete metrics they can use to evaluate their RAG/agentic frameworks and empowers them to dive deeper into the functionalities that the Ragas package offers."
  },
  "77718": {
    "title": "Evaluating the alignment of LLMs to Dutch societal values",
    "speakers": [
      "Iva Gornishka",
      "Laurens Samson"
    ],
    "type": "Talk",
    "abstract": "The City of Amsterdam is researching the responsible adoption of Large Language Models (LLMs) by evaluating their performance, environmental impact, and alignment with human values. In this talk, we will share how we develop tailored benchmarks and a dedicated assessment platform to raise awareness and guide responsible implementation.",
    "description_text": "The City of Amsterdam is known for continuously experimenting with AI and its commitment to the responsible adoption of emerging technologies. Naturally, this includes a strong focus on the use of Large Language Models (LLMs) to make municipal processes and services not only more efficient but also more transparent, inclusive, and user-friendly. However, while LLMs offer plentiful opportunities, they also come with significant risks. One of the most critical decisions in designing LLM systems is the choice of an underlying model - one that aligns with the needs and values of the City and its citizens. But what are these values? And how do we ensure that LLMs meet them? To address these questions, our team has been working to empower the City of Amsterdam and inspire the Dutch government to responsibly implement LLMs by creating a comprehensive overview of their performance, environmental impact, and alignment with human values. In this talk, we will share our ethical considerations when employing LLMs and walk you through the aspects that experts and users find most important to evaluate. We will discuss how we translate these values into measurable benchmarks and curate datasets tailored to Dutch governmental needs. Finally, we will showcase our dedicated LLM benchmarking platform, which can be used to assess LLMs and inspire others to adopt them responsibly."
  },
  "77613": {
    "title": "Help! There Are Humans in My Data!",
    "speakers": [
      "Marysia Winkels",
      "Isabelle Donatz-Fest"
    ],
    "type": "Talk",
    "abstract": "Good quality data is the basis for high quality models and valuable data insights. But isn't it annoying how often your data is riddled with those pesky humans? Human involvement in data creation often introduces errors, misunderstandings, and biases that can compromise data integrity. This talk will explore how human factors influence the data creation process and what we as data professionals can do to account for this in our data interpretation and usage.",
    "description_text": "Good quality data is the basis for good quality models and valuable data insights. But isn't it annoying how often your data is riddled with those pesky humans? A website visitor that accidentally clicks on an ad while scrolling through a website, a call center agent whose summaries grow shorter and more vague as the workday drags on, a data annotator desperately in need of a coffee, or an officer faced with crime categories that just don\u2019t seem to fit so they just click \"other\". They all create noise in our data, that we just need to get rid of. But is it really that simple? This talk argues that data creation is inherently human work, and as data professionals, we need to be aware of the human factors in this in order to properly interpret our data. We will first explore this through a case study of datafication of street-level policing within the Netherlands Police, where we'll see how human factors influence structured and unstructured reporting. Next, we will examine how we - as data professionals - can effectively account for these human factors, both technically in data processing and modelling, as well as in the overall approach to data collection. This talk is exceptionally well-suited for data professionals who work with 'ready-to-go' datasets that they have not been involved in creating, as well as those who have a role in designing the processes for data collection within their company."
  },
  "78215": {
    "title": "Real-Time Context Engineering for LLMs",
    "speakers": [
      "Manu Joseph"
    ],
    "type": "Talk",
    "abstract": "Context engineering has replaced prompt engineering as the main challenge in building agents and LLM applications. Context engineering involves providing LLMs with relevant and timely context data from various data sources, which allows them to make context-aware decisions. The context data provided to the LLM must be produced in real-time to enable it to react intelligently at human perceivable latencies (a second or two at most). If the application takes longer to react, humans would perceive it as laggy and unintelligent. In this talk, we will introduce context engineering and motivate for real-time context engineering for interactive applications. We will also demonstrate how to integrate real-time context data from applications inside Python agents using the Hopsworks feature store and corresponding application IDs. Application IDs are the key to unlock application context data for agents and LLMs. We will walk through an example of an interactive application (TikTok clone) that we make AI-enabled with Hopsworks.",
    "description_text": "This talk would also discuss practical methods for embedding real-time application data into Python-based agents. There will also be a live demonstration of an interactive AI-powered app (a TikTok-style clone) that showcases how timely context transforms user experience. Topics covered include : - Context Engineering for Agents - LLMs - Real time feature retrieval - Architecture + orchestration lessons - Building Scalable AI systems Key takeaways: - Context engineering is critical for AI Applications using LLMs. - Strict latency requirements need to be maintained for interactive applications."
  },
  "77055": {
    "title": "Lightning Talks",
    "speakers": [],
    "type": "Lightning Talks",
    "abstract": "Lightning Talks",
    "description_text": "Lightning Talks"
  },
  "77532": {
    "title": "Minus Three Tier: Data Architecture Turned Upside Down",
    "speakers": [
      "Hannes M\u00fchleisen"
    ],
    "type": "Keynote",
    "abstract": "Every data architecture diagram out there makes it abundantly clear who's in charge: At the bottom sits the analyst, above that is an API server, and on the very top sits the mighty data warehouse. This pattern is so ingrained we never ever question its necessity, despite its various issues like slow data response time, multi-level scaling issues, and massive cost. But there is another way: Disconnect of storage and compute enables localization of query processing closer to people, leading to much snappier responses, natural scaling with client-side query processing, and much reduced cost. In this talk, it will be discussed how modern data engineering paradigms like decomposition of storage, single-node query processing, and lakehouse formats enable a radical departure from the tired three-tier architecture. By inverting the architecture we can put user's needs first. We can rely on commoditised components like object store to enable fast, scalable, and cost-effective solutions.",
    "description_text": "Wait for all the details in the coming days."
  },
  "77054": {
    "title": "Conference closing notes",
    "speakers": [],
    "type": "Closing Notes Friday",
    "abstract": "Conference closing notes",
    "description_text": "Conference closing notes"
  },
  "79497": {
    "title": "Techie vs Comic: The sequel",
    "speakers": [
      "Arda Kaygan"
    ],
    "type": "Talk",
    "abstract": "A data scientist by day and a standup comedian by night. This was how Arda described himself prior to his critically acclaimed performance about his two identities during PyData 2024, where they merged. Now he doesn't even know. After another year of stage performances, awkward LinkedIn interactions and mysterious cloud errors, Arda is back for another tale of absurdity. In this closing talk, he will illustrate the hilarity of his life as a data scientist in the age of LLMs and his non-existent comfort zone, providing good sequels can exist",
    "description_text": "."
  },
  "77593": {
    "title": "Model Context Protocol: Principles and Practice",
    "speakers": [
      "Fabio Lipreri",
      "Gabriele Orlandi"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "Large\u2011language\u2011model agents are only as useful as the *context* and *tools* they can reach. Anthropic\u2019s **Model Context Protocol (MCP)** proposes a universal, bidirectional interface that turns every external system\u2014SQL databases, Slack, Git, web browsers, even your local file\u2011system\u2014into first\u2011class \u201ccontext providers.\u201d In just 30 minutes we\u2019ll step from high\u2011level buzzwords to hands\u2011on engineering details: - How MCP\u2019s JSON\u2011RPC message format, streaming channels, and version\u2011negotiation work under the hood. - Why per\u2011tool sandboxing via isolated client processes hardens security (and what happens when an LLM tries `rm \u2011rf /`). - Techniques for hierarchical context retrieval that stretch a model\u2019s effective window beyond token limits. - Real\u2011world patterns for accessing multiple tools\u2014Postgres, Slack, GitHub\u2014and plugging MCP into GenAI applications. Expect code snippets and lessons from early adoption. You\u2019ll leave ready to wire your own services into any MCP\u2011aware model and level\u2011up your GenAI applications\u2014without the N\u00d7M integration nightmare.",
    "description_text": "Large\u2011language\u2011model agents are only as useful as the *context* and *tools* they can reach. Anthropic\u2019s **Model Context Protocol (MCP)** proposes a universal, bidirectional interface that turns every external system\u2014SQL databases, Slack, Git, web browsers, even your local file\u2011system\u2014into first\u2011class \u201ccontext providers.\u201d In just 30 minutes we\u2019ll step from high\u2011level buzzwords to hands\u2011on engineering details: - How MCP\u2019s JSON\u2011RPC message format, streaming channels, and version\u2011negotiation work under the hood. - Why per\u2011tool sandboxing via isolated client processes hardens security (and what happens when an LLM tries `rm \u2011rf /`). - Techniques for hierarchical context retrieval that stretch a model\u2019s effective window beyond token limits. - Real\u2011world patterns for accessing multiple tools\u2014Postgres, Slack, GitHub\u2014and plugging MCP into GenAI applications. Expect code snippets and lessons from early adoption. You\u2019ll leave ready to wire your own services into any MCP\u2011aware model and level\u2011up your GenAI applications\u2014without the N\u00d7M integration nightmare. ### **Schedule** **0 \u2013 5 mins:** *The integration headache* \u2014 why function\u2011calling alone can\u2019t scale. **5 \u2013 15 mins:** *Under the Hood* \u2014 MCP architecture, JSON\u2011RPC anatomy, bidirectional streaming & security sandboxing. **15 \u2013 25 mins:** *Orchestrating Tools* \u2014 patterns and multi\u2011tool workflows, with code examples. **25 \u2013 30 mins:** *Ecosystem & Q/A* \u2014 pre\u2011built servers, SDKs, when MCP is overkill."
  },
  "79195": {
    "title": "Detection of Unattended Objects in Public Spaces using AI",
    "speakers": [
      "Evertjan Peer"
    ],
    "type": "Talk",
    "abstract": "This talk presents an end-to-end solution for detecting unattended objects in public transport hubs to enhance social security. The project, developed in a three-week challenge, focuses on proactively identifying unattended items using existing camera infrastructure. We will cover the entire pipeline, from data anonymization and preprocessing to building a data labeling platform, object detection with YOLO, and tracking objects over time. The presentation will also discuss the evaluation of the system.",
    "description_text": "This session details a project to develop an AI-powered system that automatically detects unattended baggage in busy public environments. We will walk through the challenges and solutions of building a complete detection pipeline, from handling privacy-sensitive video data to training a custom model that can perform effectively in a complex real-world setting. The talk will cover the practical steps taken to build and evaluate the system."
  },
  "77780": {
    "title": "Optimal Observability: Partitioning Data into Time-Series for Enhanced Anomaly Detection and Improved Monitoring Coverage",
    "speakers": [
      "Vitalie Spinu"
    ],
    "type": "Talk",
    "abstract": "This talk presents a principled methodology for partitioning item-level data into homogeneous time-series, with the objective of maximizing monitoring coverage and improving the detection of anomalies and drifts. We discuss the theoretical underpinnings of clustering algorithms for this task and describe practical algorithms enabling efficient search for optimal partitioning. We exemplify our approach with a real-world application in large-scale monitoring environments from the online payment domain.",
    "description_text": "In time-series monitoring, item-level data - such as clicks, transactions, or service requests - is typically aggregated into pre-defined groups based on measurable categorical features (e.g., country, operating system, integration version). The resulting time series are then analyzed for significant shifts in distribution with anomaly and drift detection pipelines. However, when the dimensionality or cardinality of the available categorical features is high, the naive stratification based on a pre-defined list of features often leads to a highly imbalanced distribution of the sizes of the time-series: a small subset of time-series contains the vast majority of observations, while the remainder comprises series with very few data points. Time-series based on a small number of items are unsuitable for reliable monitoring and are generally excluded from further analysis, resulting in incomplete coverage of the underlying item space. Moreover, heuristic or static grouping strategies frequently fail to produce homogeneous series in terms of the monitored metrics. Ideally, items exhibiting similar behavioral characteristics \u2014 those prone to fail or shift together \u2014 should be grouped within the same time-series. Without a principled approach, disparate items may be inappropriately grouped, resulting in noisy, non-informative, or misleading time-series. This presentation introduces a systematic framework for learning an \u201coptimal\u201d hierarchy of categorical features for stratification. We begin by formally defining the framework and optimization objectives, followed by a comparative analysis of greedy and non-greedy algorithms designed to solve the partitioning problem efficiently. Given the computational intractability of exhaustively searching over all feature permutations, the described methods emphasize scalability and principled feature selection. To illustrate the efficacy of our approach, we will present case studies from the payments domain, demonstrating how we reduced the number of monitored time-series from millions of heterogeneous streams to thousands of homogeneous time-series, simultaneously increasing the coverage and detection power."
  },
  "77745": {
    "title": "Searching for My Next Chart",
    "speakers": [
      "Muhammad Chenariyan Nakhaee"
    ],
    "type": "Talk",
    "abstract": "# Abstract As a data visualization practitioner, I frequently draw inspiration from the diverse and rapidly expanding community, particularly through challenges like #TidyTuesday. However, the sheer volume of remarkable visualizations quickly overwhelmed my manual curation methods\u2014from Pinterest boards to Notion pages. This created a significant bottleneck in my workflow, as I found myself spending more time cataloging charts than actively creating them. In this talk, I will present a **RAG (Retrieval Augmented Generation) based retrieval system** that I designed specifically for data visualizations. I will detail the methodology behind this system, illustrating how I addressed my own workflow inefficiencies by transforming a dispersed collection of charts into a semantically searchable knowledge base. This project serves as a practical example of applying advanced AI techniques to enhance creative technical work, demonstrating how a specialized retrieval system can significantly improve the efficiency and quality of data visualization creation process.",
    "description_text": "## Description My professional work revolves around Python, but my creative passion lies in crafting data visualizations with R and ggplot2, where I regularly participate in community challenges like #TidyTuesday and #30DayChartChallenge. Like many visualization creators, I draw inspiration from the community's collective work\u2014studying their geometric choices, color palettes, thematic elements, and annotation techniques. I particularly learn from examining contributor code on GitHub for #TidyTuesday challenges. Over time, I've tried various approaches to organize this inspiration: Pinterest boards, manual tagging in Notion pages, Twitter bookmarks, and screenshots. However, the sheer volume of remarkable visualizations being produced has overwhelmed my manual curation processes, and I realized I'm spending more time cataloging charts than actually creating them. The limitations of my conventional organization methods became clear because: - They required tedious manual tagging and organization. - They lacked any semantic search capabilities to find visualizations. - They existed in disconnected silos, making comprehensive searches impossible. - They could not scale with the exponentially growing visualization community. As an AI engineer by day, I realized this challenge was a perfect use case for a RAG-based retrieval system. By creating an intelligent search tool specifically designed for data visualizations, I could transform my scattered collection of inspiration into a searchable knowledge base. Instead of scrolling through endless saved charts or GitHub repositories, I could simply search with natural language queries like, \"I'm looking for a geofaceted map of Europe on a dark background,\" or, \"Show me circular charts that use a sans-serif font in their title.\" Implementing RAG for this domain proved particularly challenging because I'm dealing primarily with images. Simply embedding visual features from visualizations didn't yield satisfactory results. I needed to develop a hybrid approach that effectively processes both textual elements (code, descriptions, titles) and visual components. In this talk, I'll present my methodology for distilling data visualizations into searchable embeddings that capture both visual characteristics and conceptual elements. This project brings together AI and data visualization\u2014two areas that rarely meet. Though it uses RAG technology, the main goal is to help create better visualizations more easily. What began as fixing my own workflow problem could benefit many others in the visualization community. It's a practical example of how different technologies and tools can significantly improve the efficiency and quality of the data visualization creation process. --- ## Time Breakdown - **My Personal Struggle with Viz Inspiration (5 min):** Setting the stage by discussing the challenge of managing diverse data visualization inspiration, the overwhelming volume of community-shared data visualizations, and the limitations of manual curation. - **Deconstructing Visualizations for AI (7 min):** How we can break down data visualizations into searchable components, drawing parallels with the \"Grammar of Graphics\" to inform our AI-driven approach. - **Building a Hybrid RAG System: Strategies & Challenges (8 min):** Dive into the practicalities of creating a RAG system for visual content, focusing on the hybrid embedding techniques that combine image and text features and the hurdles faced. - **Live Demo: Searching for My Next Chart (5 min):** A live demonstration of the visualization search application in action, showcasing natural language queries and retrieved results. - **Key Takeaways & Future Directions (5 min):** Summarizing the main learnings, discussing future development ideas, and broader applications of this AI-powered approach. --- ## Target Audience This talk is ideal for data visualization practitioners or creatives who are looking for smart ways to organize and improve their workflows, or AI engineers exploring practical applications of RAG systems on visual data. --- ## Specialized Tracks - Data Science & Analytics - Machine Learning & Deep Learning"
  },
  "77586": {
    "title": "Is Prompt Engineering Dead? How Auto-Optimization is Changing the Game",
    "speakers": [
      "Iryna Kondrashchenko",
      "Oleh Kostromin"
    ],
    "type": "Talk",
    "abstract": "The rise of LLMs has elevated prompt engineering as a critical skill in the AI industry, but manual prompt tuning is often inefficient and model-specific. This talk explores various automatic prompt optimization approaches, ranging from simple ones like bootstrapped few-shot to more complex techniques such as MIPRO and TextGrad, and showcases their practical applications through frameworks like DSPy and AdalFlow. By exploring the benefits, challenges, and trade-offs of these approaches, the attendees will be able to answer the question: is prompt engineering dead, or has it just evolved?",
    "description_text": "With the rise of LLMs, prompt engineering has become a highly impactful skill in the AI industry. However, manual prompt tuning is challenging, time-consuming, and not always generalizable across different models. This raises a reasonable question: can prompts be automatically learned from data? The answer is yes, and in this talk, we will explore how. First, we will provide a high-level overview of various prompt optimization approaches, starting with a simple technique like bootstrapped few-shot, which automatically generates and selects an optimal set of demonstrations for each step in the LLM chain. Then, we will discuss more complex approaches, such as MIPRO and TextGrad, which directly optimize the instructions. Afterwards, we will move on to a more practical part by showcasing how these techniques can be used via popular frameworks such as DSPy and AdalFlow. Finally, we will discuss the benefits and trade-offs of these approaches and frameworks in terms of costs, complexity and performance, so the audience can decide whether prompt engineering is truly dead. Outline: - Introduction (2 min) - Discussion of problems with manual prompt engineering (3 min) - Overview of existing prompt optimization approaches (10 min): - Bootstrapped few-shot (3 min) - MIPRO (3 min) - TextGrad (4 min) - Showcasing the prompt optimization frameworks (10 min): - DSPy (5 min) - AdalFlow (5 min) - Comparison of methods and concluding remarks (5 min) This talk will be particularly interesting for people working with LLMs. Some familiarity with the domain will be beneficial."
  },
  "77559": {
    "title": "Sieves: Plug-and-Play NLP Pipelines With Zero-Shot Models",
    "speakers": [
      "Raphael Mitsch"
    ],
    "type": "Talk",
    "abstract": "Generative models are dominating the spotlight lately - and rightly so. Their flexibility and zero-shot capabilities make it incredibly fast to prototype NLP applications. However, one-shotting complex NLP problems often isn't the best long-term strategy. Decomposing problems into modular, pipelined tasks leads to better debuggability, greater interpretability, and more reliable performance. This modular pipeline approach pairs naturally with zero- and few-shot (ZFS) models, enabling rapid yet robust prototyping without requiring large datasets or fine-tuning. Crucially, many real-world applications need structured data outputs\u2014not free-form text. Generative models often struggle to consistently produce structured results, which is why enforcing structured outputs is now a core feature across contemporary NLP tools (like Outlines, DSPy, LangChain, Ollama, vLLM, and others). For engineers building NLP pipelines today, the landscape is fragmented. There\u2019s no single standard for structured generation yet, and switching between tools can be costly and frustrating. The NLP tooling landscape lacks a flexible, model-agnostic solution that minimizes setup overhead, supports structured outputs, and accelerates iteration. Introducing **Sieves**: a modular toolkit for building robust NLP document processing pipelines using ZFS models.",
    "description_text": "In NLP, implementing pipelines of established tasks is a common practice. Prototypes that require little to no data are often very valuable during early system development, as fully supervised models typically demand extensive annotated datasets. Generative and predictive zero-/few-shot (ZFS) learning models address this limitation by eliminating the immediate need for full annotation, enabling a \u201cwarm start\u201d over a \u201ccold start\u201d with fully supervised models. This approach accelerates development and enables faster feedback loops. Despite the richness of the open NLP ecosystem, there currently is no toolkit supporting rapid task prototyping with ZFS models without tight coupling to a specific (1) model, (2) structured generation library (e.g. https://github.com/stanfordnlp/dspy, https://github.com/dottxt-ai/outlines, https://github.com/1rgs/jsonformer, https://github.com/BoundaryML/baml) or (3) task. All three components should be as interchangeable as possible to improve on the developer experience and efficacy. sieves aims to fill this gap. ## Agenda This talk will cover the following topics: - **The Value of Pipelines and Task Isolation**: Exploring their usefulness as abstractions, even when using generative models. - **Warm Starts With Prototyping**: How to iterate effectively without relying on a training set. - **Overview of Structured Generation Tools**: A summary of the current landscape in structured generation. - **Introducing Sieves**: Bringing it all together with pipelines, warm starts/prototyping, and structured generation. The largest part of the presentation will focus on sieves, its core concepts, and its benefits: - **Loose coupling**: Decoupling models, tasks, and structured generation backends for flexibility. - **Unified I/O format**: Standardized input/output formats across models, tasks, and structured generation backends. - **Generative and predictive ZFS support**: Compatibility with both approaches for versatility. - **Document- and pipeline-based architecture**: Designed for maintainability and easy debugging. - **Comprehensive task support**: Comes with plenty of task implementations for ZFS regimes, enabling single-line execution for tasks such as: vlassification, NER, entity linking, guided information extraction, REL, translation, summarization, etc. - **Auxiliary component integration**: Support for essential tools often needed in practical NLP workflows, e.g. for file parsing and chunking. - **Customizability**: Lightweight encapsulation of tasks, allowing users to easily customize or add new tasks. - **Supervised training support**: Enables compilation and export of training, validation, and test datasets in popular formats like Hugging Face\u2019s datasets to simplify model distillation on LLM output. ## Takeaways By the end of this talk, attendees will understand: - **The Benefits of Pipelines and Prototyping For Warm Starts**: How pipelines and warm starts - leveraging prototypes with ZFS techniques - can speed up the development of NLP applications, also when using generative models. - **The Role of Structured Generation in Building Real-World NLP applications**: Insights into the current state of structured generation for text models and its impact on task implementation. - **Contributions of Sieves**: The place of sieve in the NLP ecosystem and how it contributes to a smoother and more productive developer experience in building out NLP applications by simplifying prototyping and accelerating development."
  },
  "77762": {
    "title": "Optimize the Right Thing: Cost-Sensitive Classification in Practice",
    "speakers": [
      "Shimanto Rahman"
    ],
    "type": "Talk (interested to lengthen to deepdive)",
    "abstract": "Not all mistakes in machine learning are equal\u2014a false negative in fraud detection or medical diagnosis can be far costlier than a false positive. Cost-sensitive learning helps navigate these trade-offs by incorporating error costs into the training process, leading to smarter decision-making. This talk introduces Empulse, an open-source Python package that brings cost-sensitive learning into scikit-learn. Attendees will learn why standard models fall short in cost-sensitive scenarios and how to build better classifiers with Scikit-Learn and Empulse.",
    "description_text": "The confusion matrix doesn\u2019t tell the whole story. In many real-world applications\u2014fraud detection, medical diagnosis, churn, or loan approvals\u2014the cost of a false positive can be vastly different from a false negative. Yet, traditional machine learning models optimize for overall accuracy, often ignoring these asymmetric costs and leading to poor decisions. This talk will help attendees find their way through cost-sensitive learning concepts and introduce Empulse, a powerful yet easy-to-use package that brings cost-sensitive techniques to the scikit-learn ecosystem. Empulse also homogenizes a range of fragmented cost-sensitive methods\u2014previously scattered across research papers and ad hoc implementations\u2014into a consistent, scikit-learn-compatible API. Through live examples and real-world applications, participants will learn how to integrate cost-sensitive learning into their ML workflows and avoid the pitfalls of cost-insensitive models. **Who should attend?** Data scientists, ML engineers, and researchers who want to improve model decision-making in cost-sensitive environments. Basic knowledge of scikit-learn and machine learning concepts (e.g., classification, evaluation metrics) will be helpful but not required. ### Talk outline - **Why cost-sensitive learning?** (5 min) \u2013 Why standard machine learning models fail in cost-sensitive scenarios - **Cost-sensitive learning fundamentals** (10 min) \u2013 Cost matrices, cost-sensitive metrics, models and preprocessing techniques - **Implementing Cost-sensitive techniques** (15 min) \u2013 Showing what you can already do in sklearn and how enhance it with Empulse. - **Q&A and discussion**"
  },
  "77560": {
    "title": "Declarative Feature Engineering: Bridging Spark and Flink with a Unified DSL",
    "speakers": [
      "Miguel Leite",
      "Vitalii Zhebrakovskyi"
    ],
    "type": "Talk",
    "abstract": "Building ML features at scale shouldn\u2019t require every ML Scientist to become an expert in Spark or Flink. At Adyen, the Feature Platform team built a Python-based DSL that lets data scientists define features declaratively \u2014 while automatically generating the necessary batch or real-time pipelines behind the scenes.",
    "description_text": "Adyen processes billions of payments globally, relying heavily on machine learning. As the demand for new features and faster experimentation grew across ML teams, it became clear that requiring data scientists to build and maintain their own pipelines was slowing development. To address this, we built a Python-based domain-specific language (DSL) that allows ML scientists to define features declaratively \u2014 focusing on logic, not infrastructure. Behind the scenes, the DSL translates these definitions into production-ready pipelines: Spark jobs orchestrated by Airflow for batch processing, and Flink jobs for low-latency, real-time features. Features are stored and served via HDFS and/or Cassandra, ensuring consistency across training and inference. This talk presents an architectural overview of the platform, including key design decisions and trade-offs \u2014 from the DSL\u2019s structure and code generation to deployment, orchestration, and online serving. We'll also share lessons learned from scaling this system in a high-throughput payments environment, and how close collaboration between ML scientists and engineers helped bridge the gap between experimentation and production. Co-presented by members of Adyen\u2019s ML and Platform Engineering teams, this session offers a practical look at building, operating, and evolving a robust feature platform for production-grade machine learning."
  },
  "77567": {
    "title": "Kafka Internals I Wish I Knew Sooner: The Non-Boring Truths",
    "speakers": [
      "Dima Baranetskyi"
    ],
    "type": "Talk",
    "abstract": "Most of us start with Kafka by building a simple producer/consumer demo. It just works \u2014 until it doesn\u2019t. Suddenly, disk space isn\u2019t freed up after data \u201cexpires,\u201d rebalances loop endlessly during deploys, and strange errors about missing leaders clog your logs. In the panic, we dive into Kafka\u2019s ocean of config options \u2014 hoping something will stick. Sound familiar? This talk is a collection of hard-won lessons \u2014 not flashy tricks, but the kind of insights you only gain after operating Kafka in production for years. You\u2019ll walk away with mental models that make Kafka\u2019s internal behavior more predictable and less surprising. We\u2019ll cover: - Storage internals: Why expired data doesn\u2019t always free space \u2014 and how Kafka actually reclaims disk - Transactions & delivery semantics: What \u201cexactly-once\u201d really means, and when it silently downgrades - Consumer group rebalancing: Why rebalances loop, and how the controller\u2019s hidden behavior affects them If you\u2019ve used Kafka \u2014 or plan to \u2014 these insights will save you hours of frustration and debugging. A basic understanding of partitions, replication, and Kafka\u2019s general architecture will help get the most out of this session.",
    "description_text": ""
  },
  "77574": {
    "title": "Kickstart Your Probabilistic Forecasting with Level Set and Quantile Regression Forests",
    "speakers": [
      "Inge van den Ende"
    ],
    "type": "Talk",
    "abstract": "Probabilistic forecasting is essential, but choosing the right method is tricky. This talk introduces two lesser-known models \u2014 Level Set Forecaster and Quantile Regression Forest \u2014 that help you kickstart probabilistic forecasting without unnecessary complexity.",
    "description_text": "You want a probabilistic forecast, but where do you begin? There\u2019s no single best model, and the landscape of probabilistic methods can feel overwhelming. From Bayesian models to neural networks, the variety is huge, and the learning curve steep. This talk helps you skip the theory maze and get hands-on with two lesser-known, practical models that are easy to implement with Python: - **Level Set Forecaster**: a simple wrapper that turns any existing point forecast into a probabilistic forecast using a nonparametric approach. Ideal if you already have a point model in place. - **Quantile Regression Forest**: a powerful variation of Random Forests that outputs a probabilistic forecast, offering sample-specific uncertainty estimates. Although powerful, these models are often buried in academic papers, making them hard to approach. This talk cuts through the complexity and offers a practical, implementation-focused guide you can apply right away. This talk is ideal for data scientists who are comfortable with point prediction models and want to start applying probabilistic forecasting."
  },
  "77584": {
    "title": "Orchestrating success: How Vinted standardizes large-scale, decentralized data pipelines",
    "speakers": [
      "Rodrigo Loredo",
      "Oscar Ligthart"
    ],
    "type": "Talk",
    "abstract": "At Vinted, Europe\u2019s largest second-hand marketplace, over 20 decentralized data teams generate, transform, and build products on petabytes of data. Each team utilizes their own tools, workflows, and expertise. Coordinating data pipeline creation across such diverse teams presents significant challenges. These include complex inter-team dependencies, inconsistent scheduling solutions, and rapidly evolving requirements. This talk is aimed at data engineers, platform engineers, and technical leads with experience in workflow orchestration and will demonstrate how we empower teams at Vinted to define data pipelines quickly and reliably. We will present our user-friendly abstraction layer built on top of Apache Airflow, enhanced by a Python code generator. This abstraction simplifies upgrades and migrations, removes scheduler complexity, and supports Vinted\u2019s rapid growth. Attendees will learn how Python abstractions and code generation can standardize pipeline development across diverse teams, reduce operational complexity, and enable greater flexibility and control in large-scale data organizations. Through practical lessons and real-world examples of our abstraction interface, we will offer insights into designing scheduler-agnostic architectures for successful data pipeline orchestration.",
    "description_text": "This talk will present the architectural and practical decisions behind Vinted's approach to managing large-scale, decentralized data pipelines. Aimed at data engineers, platform builders, and technical leads familiar with orchestration tools (such as Apache Airflow, Prefect, or Dagster), the session will focus on the lessons learned from building and operating our \"workflow abstraction layer,\" and the supporting code generation infrastructure that enables fast, reliable, and consistent pipeline delivery. Key session topics: 1. Background & Motivation - Describe Vinted's organizational landscape\u2014multiple product verticals, autonomous data teams, and shared infrastructure. - Surface typical challenges: duplicated pipeline code, diverging scheduler solutions, migration issues, and silos across dbt, Docker-based, and ML workflows. - Explain the need for abstraction and unification to unlock velocity and safety at scale. 2. Abstraction Layer Design - Present our user-facing abstraction API for describing pipelines, regardless of job type or runner. - Discuss how we enable standardization without constraining teams' unique needs or technical stacks. - Show how this API hides Airflow and scheduler-specific details, facilitating upgrades (e.g. Airflow 3.0, or a completely new scheduler tool). 3. Python Code Generation & Deployment - Walk through our automated code generation tool. - Generating Airflow DAGs for dbt repositories, Dockerized data jobs, and Google Vertex-AI (ML) pipelines. How configuration and validation happen at code-generation time during CI, minimizing surprises at runtime. - How we deploy interconnected DAGs independently with declarative dependencies between data assets using a global data asset registry (metadata, SLOs, and owner information). 4. Outcomes, Lessons Learned, & Future Directions - Quantifiable impact: reduced pipeline delivery times, improved reliability, more autonomous teams. - Operational benefits: easier scheduler upgrades, standardized monitoring, and simplified incident management. - Lessons on balancing central guardrails and team autonomy. - Ideas for \"scheduler-agnostic\" evolution: how our abstractions would allow us to seamlessly upgrade to Airflow 3.0 and support new runtimes (e.g. On-prem kubernetes tasks) in the future."
  },
  "77763": {
    "title": "Resource Monitoring and Optimization with Metaflow",
    "speakers": [
      "Gergely Daroczi"
    ],
    "type": "Talk",
    "abstract": "Metaflow is a powerful workflow management framework for data science, but optimizing its cloud resource usage still involves guesswork. We have extended Metaflow with a lightweight resource tracking tool that automatically monitors CPU, memory, GPU, and more, then recommends the most cost-effective cloud instance type for future runs. A single line of code can save you from overprovisioned costs or painful job failures!",
    "description_text": "Metaflow empowers data scientists with reproducible workflows, versioned artifacts, and scalable executions on AWS Batch or Kubernetes. While data scientists can request CPU, memory, etc. with the @resources Python decorator, Metaflow doesn't track what's actually tracked at runtime -- leading to overprovisioning, wasted money, or job that crash due to underestimated memory/GPU constraints. We built the @track_resources decorator to address this, which automatically profiles CPU, memory, GPU, VRAM, traffic, storage, and I/O usage at both the process-level and system-wide for each workflow step, whether running locally or in the cloud. With zero-effort and zero dependencies, this tool: - Auto-generates Metaflow Cards with temporal resource charts, per-instance-type cost projections, and cloud server recommendations. - Provides actionable insights for optimizing resource allocation. - Requires just a single line of code to integrate. Check it out on GitHub: https://github.com/SpareCores/resource-tracker"
  },
  "81753": {
    "title": "Data that Keeps Our Energy in Balance - From churn prediction with deep learning to real-time trading systems",
    "speakers": [
      "Pablo Estevez"
    ],
    "type": "Talk",
    "abstract": "This talk explores how data science helps balance energy systems in the face of demand volatility, generation volatility, and the push for sustainability. We\u2019ll dive into two technical case studies: churn prediction using survival models, and the design of a high-availability real-time trading system on Databricks. These examples illustrate how data can support operational resilience and sustainability efforts in the energy sector.",
    "description_text": "Energy may seem simple: pay a provider, receive electricity, not much data involved. But the reality is far more complex. With multiple producers, consumers, and grid operators, and the push for sustainability, balancing energy becomes a challenge in which data plays a big role. This talk explores the complexities of the energy sector, focusing on the imbalance problem: energy in must equal energy out on the grid at every single moment. We\u2019ll dive into two areas where data helps Eneco keep the grid stable while supporting society\u2019s shift toward more sustainable energy use:"
  },
  "77651": {
    "title": "Composable Pipelines for ML: Automating Feature Engineering with Hopsworks\u2019 Brewer",
    "speakers": [
      "Javier de la R\u00faa Mart\u00ednez"
    ],
    "type": "Talk",
    "abstract": "Operationalizing ML isn\u2019t just about models \u2014 it\u2019s about moving and engineering data. At Hopsworks, we built a composable AI pipeline builder (Brewer) based on two principles: Tasks and Data Sources. This lets users define workflows that automatically analyse, clean, create and update feature groups, without glue code or brittle scheduling logic. In this talk, we\u2019ll show how Brewer drives the automation of feature engineering, enabling reproducible, declarative pipelines that respond to changes in upstream data. We\u2019ll explore how this fits into broader ML workflows, from ingestion to feature materialization, and how it integrates with warehouses, streams, and file-based systems.",
    "description_text": "We\u2019ll also unpack the real challenges: triggering logic, metadata management, integration with orchestration engines, and maintaining transparency when LLMs are involved. Yes \u2014 there will be diagrams and code. Topics covered include: - Tasks and Connectors in feature pipelines - Automating tables updates - Integration with existing data stacks - Architecture + orchestration lessons - Scaling reproducibility with metadata Key takeaways: - Feature engineering deserves real automation - LLMs in ML workflows - Metadata is a first-class citizen - Clean abstractions beats MCP"
  },
  "80910": {
    "title": "Open source sprints - PyIceberg & PyMC",
    "speakers": [
      "Fokko Driesprong",
      "Rob Zinkov"
    ],
    "type": "Sprint",
    "abstract": "Also this year, at our 10 year anniversary edition of PyData Amsterdam, we\u2019ll host open source sprints! \ufe0f Our open source sprints this year will be 2 sessions in parallel, with leading open source contributors Fokko Driesprong and Rob Zinkov of the respective packages PyIceberg and PyMC.",
    "description_text": "What is an open source sprint? An open source sprint is an event where programmers come together to make focused contributions to an open source project within a limited timeframe. Participants can work individually or in groups, collaborate directly on coding tasks, fix bugs, and develop new features for the project. The sprint environment offers hands-on coding, mentorship from experienced contributors, and fosters networking and community building among peers. Through time-boxed sessions, programmers can learn from each other, solve problems collaboratively, and quickly advance project goals in a welcoming, supportive setting. Why should you join? - Gain practical coding experience on real-world projects - Receive guidance and mentorship from skilled contributors - Make a meaningful impact on an open source project - Expand your professional network and meet like-minded peers - Learn best practices and new technologies in a supportive environment"
  }
}